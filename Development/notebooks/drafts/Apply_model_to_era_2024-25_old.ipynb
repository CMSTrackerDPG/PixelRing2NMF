{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7236284d",
   "metadata": {},
   "source": [
    "# Finding Anomalies for an Entire Era\n",
    "This notebook is for compiling an Excel file detailing all of the anomalies in a single era. \n",
    "\n",
    "Specify the Era\n",
    "\n",
    "1. Load the appropriate model\n",
    "2. Import all runs/lumisections that pass the DCS flags\n",
    "3. Predict on the lumisections\n",
    "4. Loop over each run\n",
    "    * Split the long combined lumisection, data, and predictions arrays with their respective run number\n",
    "    * Store the predictions with the specific run in a dictionary\n",
    "    * Keep a running list of each dictionary\n",
    "5. Loop over each run\n",
    "    * Calculate the losses and binary losses (Do this in separate loop so we can change the loss threshold)\n",
    "6. Loop over each run and analyze the anomalies\n",
    "    * Try to figure out a data storage format that we can run through the normal Excel creation format\n",
    "7. Create an plot for each anomalous lumisection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a85f80a1",
   "metadata": {
    "init_cell": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The skip_kernel_extension extension is already loaded. To reload it, use:\n",
      "  %reload_ext skip_kernel_extension\n",
      "CPU times: user 9.22 ms, sys: 4.57 ms, total: 13.8 ms\n",
      "Wall time: 1.29 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# imports\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import joblib\n",
    "import importlib\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import plottools as plottools\n",
    "\n",
    "from nmf2d import NMF2D\n",
    "\n",
    "import functions\n",
    "importlib.reload(functions);\n",
    "from functions import *\n",
    "    \n",
    "#Used to load an extension that can skip the remaining execution of a cell. \n",
    "#Used to skip the training so we don't constantly retrain a model\n",
    "%load_ext skip_kernel_extension\n",
    "%reload_ext skip_kernel_extension\n",
    "\n",
    "optimized_powerGroupStringsList = np.array(['FPix_BmO_D3_ROG4','FPix_BmO_D2_ROG4','FPix_BmO_D1_ROG4','FPix_BmO_D3_ROG3','FPix_BmO_D2_ROG3','FPix_BmO_D1_ROG3','FPix_BmO_D3_ROG2','FPix_BmO_D2_ROG2','FPix_BmO_D1_ROG2','FPix_BmO_D3_ROG1','FPix_BmO_D2_ROG1','FPix_BmO_D1_ROG1','FPix_BmI_D3_ROG1','FPix_BmI_D2_ROG1','FPix_BmI_D1_ROG1','FPix_BmI_D3_ROG2','FPix_BmI_D2_ROG2','FPix_BmI_D1_ROG2','FPix_BmI_D3_ROG3','FPix_BmI_D2_ROG3','FPix_BmI_D1_ROG3','FPix_BmI_D3_ROG4','FPix_BmI_D2_ROG4','FPix_BmI_D1_ROG4','FPix_BpO_D1_ROG4','FPix_BpO_D2_ROG4','FPix_BpO_D3_ROG4','FPix_BpO_D1_ROG3','FPix_BpO_D2_ROG3','FPix_BpO_D3_ROG3','FPix_BpO_D1_ROG2','FPix_BpO_D2_ROG2','FPix_BpO_D3_ROG2','FPix_BpO_D1_ROG1','FPix_BpO_D2_ROG1','FPix_BpO_D3_ROG1','FPix_BpI_D1_ROG1','FPix_BpI_D2_ROG1','FPix_BpI_D3_ROG1','FPix_BpI_D1_ROG2','FPix_BpI_D2_ROG2','FPix_BpI_D3_ROG2','FPix_BpI_D1_ROG3','FPix_BpI_D2_ROG3','FPix_BpI_D3_ROG3','FPix_BpI_D1_ROG4','FPix_BpI_D2_ROG4','FPix_BpI_D3_ROG4'])\n",
    "#A list of all of the quarters of the detector\n",
    "QUARTERS = np.array([['FPix_BmI_D3_ROG1','FPix_BmI_D3_ROG2','FPix_BmI_D3_ROG3','FPix_BmI_D3_ROG4','FPix_BmI_D2_ROG1','FPix_BmI_D2_ROG2','FPix_BmI_D2_ROG3','FPix_BmI_D2_ROG4','FPix_BmI_D1_ROG1','FPix_BmI_D1_ROG2','FPix_BmI_D1_ROG3','FPix_BmI_D1_ROG4'], ['FPix_BmO_D3_ROG1','FPix_BmO_D3_ROG2','FPix_BmO_D3_ROG3','FPix_BmO_D3_ROG4','FPix_BmO_D2_ROG1','FPix_BmO_D2_ROG2','FPix_BmO_D2_ROG3','FPix_BmO_D2_ROG4','FPix_BmO_D1_ROG1','FPix_BmO_D1_ROG2','FPix_BmO_D1_ROG3','FPix_BmO_D1_ROG4'], ['FPix_BpI_D1_ROG1','FPix_BpI_D1_ROG2','FPix_BpI_D1_ROG3','FPix_BpI_D1_ROG4','FPix_BpI_D2_ROG1','FPix_BpI_D2_ROG2','FPix_BpI_D2_ROG3','FPix_BpI_D2_ROG4','FPix_BpI_D3_ROG1','FPix_BpI_D3_ROG2','FPix_BpI_D3_ROG3','FPix_BpI_D3_ROG4'], ['FPix_BpO_D1_ROG1','FPix_BpO_D1_ROG2','FPix_BpO_D1_ROG3','FPix_BpO_D1_ROG4','FPix_BpO_D2_ROG1','FPix_BpO_D2_ROG2','FPix_BpO_D2_ROG3','FPix_BpO_D2_ROG4','FPix_BpO_D3_ROG1','FPix_BpO_D3_ROG2','FPix_BpO_D3_ROG3','FPix_BpO_D3_ROG4']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9104f71",
   "metadata": {},
   "source": [
    "|        2024 Era     | C | D | E | E | F |   F  |   G  |   H  |   I  |   I  |\n",
    "|:-------------------:|:-:|:-:|:-:|:-:|:-:|:----:|:----:|:----:|:----:|:----:|\n",
    "|       Version       | 1 | 1 | 1 | 2 | 1 |   1  |   1  |   1  |   1  |   2  |\n",
    "|        Period       | 1 | 1 | 1 | 1 | 1 |   2  |   2  |   2  |   2  |   2  |\n",
    "|        Model        | 1 | 1 | 1 | 1 | 1 |   2  |   2  |   2  |   2  |   2  |\n",
    "\n",
    "|        2025 Era     | C | C | C | D | E | F |   G  |   H  |   I  |   J  |\n",
    "|:-------------------:|:-:|:-:|:-:|:-:|:-:|:-:|:----:|:----:|:----:|:----:|\n",
    "|       Version       | 1 | 1 | 2 | 1 | 1 | 1 |   1  |   1  |   1  |   1  |\n",
    "|        Period       | 3 | 4 | 4 | X | X | X |   X  |   X  |   X  |   X  |\n",
    "|        Model        | 3 | 4 | 4 | X | X | X |   X  |   X  |   X  |   X  |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9beaec",
   "metadata": {},
   "source": [
    "# Important Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "daead652",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the era: import and search for anomalies\n",
    "RING = 1\n",
    "YEAR, ERA, VERSION, PERIOD = 2024, \"C\", 1, 1 \n",
    "\n",
    "number, type = 1, 1\n",
    "model_name = f'model_{number}_PXRing_{RING}_period_{PERIOD}_type_{type}.pkl'\n",
    "#Folder --> /eos/user/a/alaperto/SWAN_projects/NMFolder/models/\n",
    "#Data by Lyka --> /eos/user/l/llambrec/dialstools-output/\n",
    "#Original Jake --> /eos/user/j/jomorris/SWAN_projects/NMF Testing/\n",
    "\n",
    "# Model thresholds\n",
    "EDITION = 1\n",
    "if YEAR == 2024:\n",
    "    LOSS_THRESHOLD = 4e5 #Threshold on Loss of the ROC --> Define ROC as Bad\n",
    "elif YEAR == 2025:\n",
    "    LOSS_THRESHOLD = 9e5\n",
    "ANOMALY_CUTOFF = 40 #Threshold on fraction of powergroup that is Bad --> Define LS as anomalous\n",
    "\n",
    "#Plotting Globals\n",
    "DO_PLOTTING = False #Whether or not to plot EVERY anomalous lumisection in this era. WARNING: Takes a long time. ~13mins for 93 anomalies\n",
    "SAVE_FIGS = False #Whether or not to also SAVE the plot of every anomalous lumisection. DO_PLOTTING must also be True for the images to be saved\n",
    "\n",
    "#Some light calculation of important variables\n",
    "file = f'../data/ZeroBias-Run{YEAR}{ERA}-PromptReco-v{VERSION}-DQMIO-PixelPhase1-Phase1_MechanicalView-PXForward-clusters_per_SignedDiskCoord_per_SignedBladePanelCoord_PXRing_{RING}.parquet'\n",
    "oms_json = f'../omsdata/omsdata_Run{YEAR}{ERA}-v{VERSION}.json'\n",
    "#ring_num = int(file[-9]) #The -9th character is ALWAYS the ring number for our data\n",
    "ring_num = RING\n",
    "\n",
    "#The directory name to use for \n",
    "DIR_NAME = f\"../results/output_{YEAR}{ERA}_v{VERSION}_PXRing_{RING}_edition_{EDITION}\" \n",
    "if not os.path.exists(DIR_NAME): os.makedirs(DIR_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2b5c3d",
   "metadata": {},
   "source": [
    "## 1. Load the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b439243b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Model: ../models/model_1_PXRing_1_period_1_type_1.pkl\n",
      "Model Shape: [88, 48]\n"
     ]
    }
   ],
   "source": [
    "nmf_file = f'../models/{model_name}'\n",
    "nmf = joblib.load(nmf_file)\n",
    "\n",
    "print(f\"Loaded Model: {nmf_file}\")\n",
    "print(f\"Model Shape: {nmf.xshape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6acd06e",
   "metadata": {},
   "source": [
    "## 2. Import all runs/lumisections that pass the DCS flags\n",
    "Import the entire era at once so we only have to go to the disk once. \n",
    "\n",
    "Use the OMS JSON to filter only the lumisections that pass the DCS flags. Helps to reduce unnecessary predictions on bad lumisections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cd95e8c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 42278 rows in the Parquet dataset but only 42277 rows in the merged dataframe! \n",
      "There are 1 lumisections that were thrown out due to no DCS flags present!\n",
      "\n",
      "There are 53 Runs and 25159 Lumisections that Pass All DCS Flags: \n",
      " [379415 379416 379420 379425 379433 379442 379454 379456 379470 379523\n",
      " 379524 379530 379613 379614 379615 379616 379617 379618 379660 379661\n",
      " 379729 379765 379774 379866 379984 380001 380005 380006 380007 380029\n",
      " 380030 380032 380033 380043 380049 380050 380051 380052 380053 380056\n",
      " 380066 380074 380115 380126 380127 380128 380195 380196 380197 380235\n",
      " 380236 380237 380238]\n",
      "CPU times: user 4.4 s, sys: 4.09 s, total: 8.49 s\n",
      "Wall time: 10.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Any logic for when parts of the detector are disabled. \n",
    "extra_filters = []\n",
    "#Logic for Era F period 1/2 and 2025 Era Cv1 period 3/4\n",
    "if YEAR == 2024 and ERA == \"F\" and PERIOD == 1:\n",
    "    extra_filters.append(('run_number', '<', 382799))\n",
    "elif YEAR == 2024 and ERA == \"F\" and PERIOD == 2:\n",
    "    extra_filters.append(('run_number', '>=', 382799))\n",
    "elif YEAR == 2025 and ERA == \"C\" and VERSION == 1 and PERIOD == 3:\n",
    "    extra_filters.append(('run_number', '<=', 392668))\n",
    "elif YEAR == 2025 and ERA == \"C\" and VERSION == 1 and PERIOD == 4:\n",
    "    extra_filters.append(('run_number', '>', 392668))\n",
    "    extra_filters.append(('run_number', '<', 393512))#Machine Development runs\n",
    "\n",
    "#Testing the function to import AND filter an entire era at once\n",
    "multi_lumi_data, all_runs, lumis, df = extract_data_whole_era(file, oms_json, extra_filters=extra_filters)\n",
    "del df\n",
    "all_runs, indices = np.unique(all_runs, return_index=True)\n",
    "indices = indices[1:] #The first index in indices is always 0, since the first number is always unique, so we discard that. \n",
    "print(f\"There are {len(all_runs)} Runs and {len(lumis)} Lumisections that Pass All DCS Flags: \\n\", all_runs)\n",
    "\n",
    "#Optional if you'd like to see how many TOTAL runs there are, as most runs are entirely disqualified due to DCS Flags. \n",
    "# dataset = ParquetDataset(file).read().to_pandas()\n",
    "# all_runs = np.array(dataset[\"run_number\"].unique())\n",
    "# print(f\"There are {len(all_runs)} Total Available Runs: \\n\", all_runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943618fb",
   "metadata": {},
   "source": [
    "## 3. Predict on the lumisections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccbfb5dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions Already Exist! Loading predictions from ../results/output_2024C_v1_PXRing_1_edition_1/Predictions\n",
      "Predictions with (25159, 88, 48) shape loaded from ../results/output_2024C_v1_PXRing_1_edition_1/Predictions.npy\n",
      "CPU times: user 278 ms, sys: 1.16 s, total: 1.44 s\n",
      "Wall time: 9.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Predict or not predict (30 mins for 25k LS)\n",
    "FORCE_PREDICT = False\n",
    "\n",
    "#Remove the cross so we can predict on it\n",
    "multi_lumi_data_no_cross = remove_cross(multi_lumi_data, RING)\n",
    "\n",
    "#Save predictions array to a file (since they take so long to produce)\n",
    "pred_filename = f\"{DIR_NAME}/Predictions\"\n",
    "\n",
    "if os.path.exists(pred_filename + '.npy') and FORCE_PREDICT == False:\n",
    "    print(f\"Predictions Already Exist!\")\n",
    "    print(f\"Loading predictions from {pred_filename}.npy\")\n",
    "    mes_pred = np.load(pred_filename + '.npy')\n",
    "    print(f\"Shape: {mes_pred.shape}\")\n",
    "else:\n",
    "    #If we don't already have a prediction directory then make it\n",
    "    if FORCE_PREDICT: \n",
    "        print(f\"FORCE_PREDICT is True. ---> Overwrite file at {pred_filename}.npy\")\n",
    "    else:\n",
    "        print(\"Predictions don't exist yet! Starting Prediction. \")\n",
    "\n",
    "    #Predict on the data\n",
    "    print(f'Predicting...')\n",
    "    start_time = time.time()\n",
    "    mes_pred = nmf.predict(multi_lumi_data_no_cross)\n",
    "    np.save(pred_filename, mes_pred)\n",
    "    print(f\"Done Predicting in {time.time() - start_time} Seconds!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8d5473",
   "metadata": {},
   "source": [
    "## 4. Loop over each run\n",
    "    * Split the long combined lumisection, data, and predictions arrays with their respective run number\n",
    "    * Store the predictions with the specific run in a dictionary\n",
    "    * Keep a running list of each dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3cff02fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.89 s, sys: 195 ms, total: 2.08 s\n",
      "Wall time: 2.08 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Verbose=0: No prints. Verbose=1: Some prints. Verbose>=2: Some very long prints\n",
    "verbose = 0\n",
    "\n",
    "#Split the long combined arrays of the lumisections, the data, and the predictions\n",
    "lumisections = np.split(lumis, indices)\n",
    "data_arr = np.split(multi_lumi_data_no_cross, indices)\n",
    "pred_arr = np.split(mes_pred, indices)\n",
    "\n",
    "#print(pred_arr[1].shape)\n",
    "\n",
    "#Now our data is splitted into arrays by run number, we can loop over the run numbers, generate our data_dicts and put them in a data_dict_list.\n",
    "#So it is in a format ready to be used to calculate the losses and anomalies. \n",
    "#Create a list to store all of the lumisections and predictions\n",
    "data_dict_list = list(np.empty_like(all_runs)) \n",
    "for index, run_number in enumerate(all_runs):\n",
    "    data_dict = {}\n",
    "    data_dict[\"run_number\"] = run_number\n",
    "    data_dict[\"lumisections\"] = lumisections[index]\n",
    "    data_dict[\"data\"] = data_arr[index]\n",
    "    data_dict[\"predictions\"] = pred_arr[index]\n",
    "    #Add the cross into the predictions and data\n",
    "    data_dict[\"data_cross\"] = add_cross(data_arr[index])\n",
    "    data_dict[\"predictions_cross\"] = add_cross(pred_arr[index])\n",
    "    \n",
    "    #Add this run to the data dict list\n",
    "    data_dict_list[index] = data_dict    \n",
    "    #Printing\n",
    "    if verbose>0:\n",
    "        print(f\"Run Number: {run_number}\")\n",
    "        print(f\"\\tThere are {len(lumisections[index])} Extracted Lumisections:\\n\\t{lumisections[index]}\\n\")\n",
    "    if verbose>1:\n",
    "        print(data_dict)\n",
    "        \n",
    "if verbose>0:\n",
    "    print(data_dict_list[0])\n",
    "elif verbose>1:\n",
    "    print(data_dict_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f1d220",
   "metadata": {},
   "source": [
    "## 5. Loop over each run\n",
    "    * Calculate the losses and binary losses (Do this in separate loop so we can change the loss threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a181376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.21 s, sys: 567 ms, total: 2.78 s\n",
      "Wall time: 2.78 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#Loop over each data dictionary in the list and calculate the losses and binary losses. \n",
    "#Add these to the dictionaries as we go along. \n",
    "for index, data_dict in enumerate(data_dict_list):\n",
    "    #Extract the needed info from the data_dict\n",
    "    multi_lumi_data_no_cross = data_dict[\"data\"]\n",
    "    mes_pred = data_dict[\"predictions\"]\n",
    "    \n",
    "    #Calculate losses\n",
    "    losses = np.square(multi_lumi_data_no_cross - mes_pred)\n",
    "    losses_binary = (losses > LOSS_THRESHOLD).astype(int)\n",
    "    \n",
    "    #Add the crosses back\n",
    "    losses_cross = add_cross(losses)\n",
    "    losses_binary_cross = add_cross(losses_binary)\n",
    "    \n",
    "    #Add new entries to the data_dict\n",
    "    #This will automatically update the dictionaries in the data_dict_list\n",
    "    data_dict[\"losses\"] = losses_cross\n",
    "    data_dict[\"losses_binary\"] = losses_binary_cross"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb061d02",
   "metadata": {},
   "source": [
    "## 6. Loop over each run and analyze the anomalies. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d315c89b",
   "metadata": {},
   "source": [
    "### Analyze Each Lumisection of Each Run for Anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63879827",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming results...\n",
      "Era 2024C_v1 (Run 379415 to Run 380238)\n",
      "92 Anomalous Lumisections\n",
      "CPU times: user 22.2 s, sys: 507 µs, total: 22.2 s\n",
      "Wall time: 22.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "testingtime = False\n",
    "verbose = False\n",
    "\n",
    "#Create lists for tracking the anomalous lumisections in all of the runs\n",
    "all_anomalous_runs = []\n",
    "all_anomalous_lumisections = []\n",
    "all_anomalous_powergroups = []\n",
    "\n",
    "for i, data_dict in enumerate(data_dict_list):\n",
    "    #Extract the required info from the data_dict\n",
    "    run_number = data_dict[\"run_number\"]\n",
    "    lumisections = data_dict[\"lumisections\"]\n",
    "    losses_binary_cross = data_dict[\"losses_binary\"]\n",
    "    \n",
    "    #Currently a list of anomalous lumisections and their powergroups\n",
    "    #These arrays are prevented from getting out of sync by still appending \n",
    "    #the anomalous lumisection even if that lumisection was already marked bad with a different powergroup\n",
    "    anomalous_lumisections = []\n",
    "    anomalous_powergroups = []\n",
    "\n",
    "    for index, lumisection in enumerate(lumisections):\n",
    "        if verbose: print(f\"Index: {index} \\t Lumisection: {lumisection}\")\n",
    "\n",
    "        for j, powergroup in enumerate(optimized_powerGroupStringsList):\n",
    "            #if not testingtime: print(f\"Power Group String: {powergroup}\")\n",
    "            powerGroupSlice, diskSlice = powerGroupToIndex(powergroup)\n",
    "\n",
    "            #Access each power group in each lumisection and see if more than 50% of the bins are on\n",
    "            #A bit ugly, but this was the fastest way I found. Saved about .1 seconds over saving the powergroup data to another variable. \n",
    "            if int(np.sum(losses_binary_cross[index, powerGroupSlice, diskSlice].flatten())) >= int(ANOMALY_CUTOFF/100 * losses_binary_cross[index, powerGroupSlice, diskSlice].flatten().size):\n",
    "                if verbose: print(f\"Anomalous Power Group: {powergroup} \\t in Lumisection: {lumisection} \\t with Binary Sum: {np.sum(powerGroup_data)}\")\n",
    "                all_anomalous_runs.append(run_number)\n",
    "                anomalous_lumisections.append(lumisection)\n",
    "                anomalous_powergroups.append(powergroup)\n",
    "                \n",
    "            #This is used to pull out specific lumisection and check their binary loss occupancy\n",
    "#             if run_number == 381544 and lumisection == 1861:\n",
    "#                 print(f\"Powergroup: {powergroup}\")\n",
    "#                 print(f\"Powergroup Size:{losses_binary_cross[index, powerGroupSlice, diskSlice].flatten().size}\")\n",
    "#                 print(f\"Sum of Binary Loss:{np.sum(losses_binary_cross[index, powerGroupSlice, diskSlice].flatten())}\\n\")\n",
    "\n",
    "                if verbose: save_digis_png(losses_binary_cross[index], testing_ring_num, lumisection, 1)\n",
    "    #Update the data_dict with the anomalous lumisections and powergroups\n",
    "    #I don't have a plan for them currently, but it could be useful\n",
    "    data_dict[\"anomalous_lumisections\"] = np.array(anomalous_lumisections)\n",
    "    data_dict[\"anomalous_powergroups\"] = np.array(anomalous_powergroups)\n",
    "    \n",
    "    #EXTEND the anomalous lumisections and powegroups to the ALL list. Extend keeps the list flat\n",
    "    all_anomalous_lumisections.extend(anomalous_lumisections)\n",
    "    all_anomalous_powergroups.extend(anomalous_powergroups)\n",
    "    \n",
    "# print(all_anomalous_runs)\n",
    "# print(all_anomalous_lumisections)\n",
    "# print(all_anomalous_powergroups)\n",
    "# print(data_dict_list[-2])\n",
    "num_anomalous_lumisections, all_anomalous_lumisections_unique = calcNumAnomalousLumisections(data_dict_list)\n",
    "print(f'Resuming results...')\n",
    "print(f\"Era {YEAR}{ERA}_v{VERSION} (Run {all_runs[0]} to Run {all_runs[-1]})\")\n",
    "print(f\"{num_anomalous_lumisections} Anomalous Lumisections\")\n",
    "#For some reason the above number can disagree with the sum of the Num_LS column in Excel. But they are never far off.\n",
    "#Might happen due to lumisections appearing twice in the excel file. Like when a single lumisection has both a single disk anomaly and a multi disk anomaly\n",
    "#The lumisection could be double counted. There may be more cases where this happens. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0a5e62",
   "metadata": {},
   "source": [
    "## Identify Anomaly Types\n",
    "This section will identify runs in lumisections that stay the same then compare the powergroups and see if there are any multi-disk anomalies. \n",
    "\n",
    "If there are no repeating lumisections, then it is just a single disk anomaly. \n",
    "\n",
    "### Create a file summarizing all lumisections and their anomalies\n",
    "#### Run_Number    Lumisection    PRT    Disk    Ring_Num    Anomaly_Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fdb779d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 187 ms, sys: 33.2 ms, total: 220 ms\n",
      "Wall time: 779 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Identify multi disk anomalies\n",
    "verbose = 0\n",
    "#Create a pandas dataframe that we can use to track EACH anomalous lumisection in each run\n",
    "headers = [\"Run_Number\", \"Lumisection\", \"Powergroup\", \"Disk\", \"Ring_Num\", \"Anomaly_Type\"]\n",
    "all_detailed_anomaly_df = pd.DataFrame(columns=headers)\n",
    "\n",
    "#Loop over each data dict in the data dict list\n",
    "for index, data_dict in enumerate(data_dict_list):\n",
    "    #Extract the relavant information\n",
    "    run_number = data_dict[\"run_number\"]\n",
    "    anomalous_lumisections = data_dict[\"anomalous_lumisections\"]\n",
    "    anomalous_powergroups = data_dict[\"anomalous_powergroups\"]\n",
    "    \n",
    "    #If there are no anomaous lumisections or powergroups then stop this iteration\n",
    "    if anomalous_lumisections.size == 0 or anomalous_powergroups.size == 0:\n",
    "        continue\n",
    "    \n",
    "\n",
    "    #Create dataframe of anomalous lumisections and powergroups\n",
    "    anomaly_df = pd.DataFrame({\"lumisections\": anomalous_lumisections, \"powergroups\": anomalous_powergroups})\n",
    "\n",
    "    if verbose>0: print(\"Anomaly Dataframe: \\n\", anomaly_df, '\\n')\n",
    "\n",
    "    #Create unique arrays to pare down duplicate data\n",
    "    anomalous_lumisections_unique = np.unique(anomalous_lumisections)\n",
    "\n",
    "    #Create a list of dictionaries for easier saving to text\n",
    "    dictList = np.empty_like(anomalous_lumisections_unique, dtype=dict)\n",
    "\n",
    "    detailed_anomaly_df = pd.DataFrame(columns=headers)\n",
    "\n",
    "    # print(\"AHHHHHHH\\n\", detailed_anomaly_df)\n",
    "    if verbose>0: print('-----------------------------------------')\n",
    "    #Loop over each unique lumisection\n",
    "    for index, lumisection in enumerate(anomalous_lumisections_unique):\n",
    "        #These values are the same for single/multi disk anomalies\n",
    "        dataDict = dict.fromkeys(headers)\n",
    "        dataDict[\"Run_Number\"] = run_number\n",
    "        dataDict[\"Lumisection\"] = lumisection\n",
    "        dataDict[\"Ring_Num\"] = ring_num\n",
    "\n",
    "        #Get the lumisection and all of the anomaly powergroups\n",
    "        #If there is only one powergroup, mark it Single Disk, preparing the detailed Pandas anomaly dataframe, then move on\n",
    "        #If there is multiple powergroups, iterate through each pair, breaking on the first Multi-disk anomaly after preparing the detailed Pandas anomaly dataframe\n",
    "        #If there is no Multi-disk anomaly despite there being multiple anomalies in one lumisection, prepare the detailed Pandas anomaly dataframe with EACH anomaly\n",
    "\n",
    "\n",
    "        dataframe = anomaly_df[anomaly_df[\"lumisections\"] == lumisection]\n",
    "        if verbose>0: print(dataframe, '\\n')\n",
    "        powergroups = dataframe[\"powergroups\"].to_list()\n",
    "        if verbose>0: print(f\"Powergroups: {powergroups}\")\n",
    "\n",
    "        #If there are 12 anomalous powergroups in one lumisection, check if it's a whole quarter out\n",
    "        if len(powergroups) == 12:\n",
    "            for quarter in QUARTERS:\n",
    "                #If all of the powergroups are in one quarter, then we can save and break early\n",
    "                if np.all(np.isin(powergroups, quarter)):\n",
    "                    #Fill in the data dict\n",
    "                    m_or_p, I_or_O, disk_number, part_number = analyzePowerGroupString(powergroups[0])\n",
    "                    dataDict[\"Powergroup\"] = ':'.join(powergroups) #Make a string of each powergroup separated by colons. A char not typically used in csv's. \n",
    "                    dataDict[\"Disk\"] = \"-1:-2:-3\" if disk_number < 0 else \"1:2:3\"\n",
    "                    #dataDict[\"Anomaly_Type\"] = \"Whole Quarter\"\n",
    "                    dataDict[\"Anomaly_Type\"] = \"Multi-Disk\"\n",
    "                    dataFrame = pd.Series(dataDict).to_frame().T\n",
    "                    #print(dataFrame)\n",
    "                    detailed_anomaly_df = pd.concat([detailed_anomaly_df, dataFrame])\n",
    "                    #break out of this for loop for the quarters\n",
    "                    break\n",
    "            #then continue to the next unique anomaly\n",
    "            continue\n",
    "\n",
    "        #If there is only one powergroup, mark it as a Single disk anomaly\n",
    "        if len(powergroups) == 1:\n",
    "            #Fill in the data dict\n",
    "            m_or_p, I_or_O, disk_number, part_number = analyzePowerGroupString(powergroups[0])\n",
    "            dataDict[\"Powergroup\"] = powergroups[0]\n",
    "            dataDict[\"Disk\"] = disk_number\n",
    "            dataDict[\"Anomaly_Type\"] = \"Single-Disk\"\n",
    "            #Convert the data dict to a pandas dataframe and concat it to the detailed data frame \n",
    "            #(NOTE: If there are many lumisections, it is technically more effificient to create a list of these dataDicts and concat those all at once)\n",
    "            if verbose>1: print(\"DATA DICT:\", dataDict)\n",
    "            dataFrame = pd.Series(dataDict).to_frame().T\n",
    "            #print(dataFrame)\n",
    "            detailed_anomaly_df = pd.concat([detailed_anomaly_df, dataFrame])\n",
    "            #continue to the next loop\n",
    "            continue\n",
    "\n",
    "        #If there is more than one anomalous powergroup in that lumisection and it's NOT the whole quarter out\n",
    "        all_powergroup_combos = itertools.combinations(powergroups, 2)\n",
    "        #Loop over all pairs of powergroups and search for multi-disk anomalies\n",
    "        dataDictList = [] #Create a list to store all of the possible Single Disk anomalies in case there are multiple anomalies but no Multi Disk anomaly\n",
    "        for powergroup_combo in all_powergroup_combos:\n",
    "            #print(\"ADFJNDKFN\", powergroup_combo)\n",
    "            anomaly_type = powerGroupsToAnomalyType(powergroup_combo[0], powergroup_combo[1])\n",
    "            #Extract relevant information\n",
    "            m_or_p_one, I_or_O_one, disk_number_one, part_number_one = analyzePowerGroupString(powergroup_combo[0])\n",
    "            m_or_p_two, I_or_O_two, disk_number_two, part_number_two = analyzePowerGroupString(powergroup_combo[1])\n",
    "            if anomaly_type == \"Multi-Disk\":\n",
    "                #Fill in the data dict\n",
    "                dataDict[\"Powergroup\"] = powergroup_combo[0] + ':' + powergroup_combo[1]\n",
    "                dataDict[\"Disk\"] = str(disk_number_one) + ':' + str(disk_number_two)\n",
    "                dataDict[\"Anomaly_Type\"] = \"Multi-Disk\"\n",
    "                #Convert the data dict to a pandas dataframe and concat it to the detailed data frame \n",
    "                #(NOTE: If there are many lumisections, it is technically more effificient to create a list of these dataDicts and concat those all at once)\n",
    "                if verbose>1: print(\"DATA DICT:\", dataDict)\n",
    "                dataFrame = pd.Series(dataDict).to_frame().T\n",
    "                #print(dataFrame)\n",
    "                detailed_anomaly_df = pd.concat([detailed_anomaly_df, dataFrame])\n",
    "                #break\n",
    "            else:\n",
    "                #Fill in a data dict for each anomaly\n",
    "                dataDict[\"Powergroup\"] = powergroup_combo[0]\n",
    "                dataDict[\"Disk\"] = str(disk_number_one)\n",
    "                dataDict[\"Anomaly_Type\"] = \"Single-Disk\"\n",
    "                #Convert the data dict to a pandas dataframe and concat it to the detailed data frame \n",
    "                #(NOTE: If there are many lumisections, it is technically more effificient to create a list of these dataDicts and concat those all at once)\n",
    "                if verbose>1: print(\"DATA DICT ONE:\", dataDict)\n",
    "                dataFrame = pd.Series(dataDict).to_frame().T\n",
    "                if verbose>1: print(dataFrame)\n",
    "                detailed_anomaly_df = pd.concat([detailed_anomaly_df, dataFrame])\n",
    "\n",
    "                #Fill in second data dict\n",
    "                dataDict[\"Powergroup\"] = powergroup_combo[1]\n",
    "                dataDict[\"Disk\"] = str(disk_number_two)\n",
    "                dataDict[\"Anomaly_Type\"] = \"Single-Disk\"\n",
    "                #Convert the data dict to a pandas dataframe and concat it to the detailed data frame \n",
    "                #(NOTE: If there are many lumisections, it is technically more effificient to create a list of these dataDicts and concat those all at once)\n",
    "                if verbose>1: print(\"DATA DICT TWO:\", dataDict)\n",
    "                dataFrame = pd.Series(dataDict).to_frame().T\n",
    "                #print(dataFrame)\n",
    "                detailed_anomaly_df = pd.concat([detailed_anomaly_df, dataFrame])\n",
    "            if verbose>0: print('----------------------------------------')\n",
    "        if verbose>0: print('-----------------------------------------')\n",
    "\n",
    "    #Remove all exact duplicate rows\n",
    "    detailed_anomaly_df = detailed_anomaly_df.drop_duplicates()\n",
    "    if verbose>0: print('\\n\\n')\n",
    "    #print(detailed_anomaly_df)\n",
    "    all_detailed_anomaly_df = pd.concat([all_detailed_anomaly_df, detailed_anomaly_df])\n",
    "    #ensure the directory is created\n",
    "    detailed_anomaly_df.to_excel(f\"{DIR_NAME}/Raw_PXRing_{RING}_{YEAR}{ERA}_Run_{run_number}.xlsx\", index=False, engine='openpyxl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b0ed8d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Condensing Lumisection Runs!\n",
      "Final Anomaly Excel File Saved at `Anomalies_PXRing_1_2024C_v1_period_1_edition_1.xlsx`\n",
      "CPU times: user 29 ms, sys: 5.52 ms, total: 34.5 ms\n",
      "Wall time: 106 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Identify and condense runs of consecutive lumisections for each powergroup\n",
    "condensed_df = condense_lumisection_runs(all_detailed_anomaly_df)\n",
    "# print(\"Condensed consecutive lumisection runs per powergroup:\")\n",
    "# condensed_df = condensed_df.sort_values(by='Start_LS')\n",
    "# print(condensed_df)\n",
    "print(\"Done Condensing Lumisection Runs!\")\n",
    "\n",
    "# Identify and condense overlaping powergroups for each lumisection\n",
    "condensed_df_again = condense_powergroup_overlap(condensed_df, verbose=False)\n",
    "# print(\"Condensed consecutive lumisection runs per Anomaly Type:\")\n",
    "# condensed_df_again = condensed_df_again.sort_values(by='Start_LS')\n",
    "\n",
    "# Rename columns\n",
    "renamed_df = condensed_df_again.rename(\n",
    "    columns={\n",
    "        \"Run_Number\": \"Run\",\n",
    "        \"Start_LS\": \"First LS\",\n",
    "        \"End_LS\": \"Last LS\",\n",
    "        \"Num_LS\": \"Total LS\",\n",
    "        \"Powergroup\": \"Barrel/Forward\",   # will overwrite below\n",
    "        \"Disk\": \"Layer/Disk\",\n",
    "        \"Ring_Num\": \"Row/Quarter\",\n",
    "        \"Anomaly_Type\": \"Type\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# Force Barrel/Forward to \"Forward\" for all rows\n",
    "renamed_df[\"Barrel/Forward\"] = \"Forward\"\n",
    "\n",
    "# Add \"Disk \" before the disk number (keep sign)\n",
    "renamed_df[\"Layer/Disk\"] = renamed_df[\"Layer/Disk\"].apply(lambda x: f\"Disk {x}\")\n",
    "\n",
    "# Add \"Ring \" before the ring number\n",
    "renamed_df[\"Row/Quarter\"] = renamed_df[\"Row/Quarter\"].apply(lambda x: f\"Ring {x}\")\n",
    "\n",
    "# Save to Excel\n",
    "excel_file = f\"Anomalies_PXRing_{RING}_{YEAR}{ERA}_v{VERSION}_period_{PERIOD}_edition_{EDITION}.xlsx\"\n",
    "excel_file_path = f\"{DIR_NAME}/../{excel_file}\"\n",
    "renamed_df.to_excel(excel_file_path, index=False, engine='openpyxl')\n",
    "print(f\"Final Anomaly Excel File Saved at `{excel_file}`\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c9ce5a",
   "metadata": {},
   "source": [
    "# Extra <---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2231e2a1",
   "metadata": {},
   "source": [
    "# 7. Optional (Save plot of each anomalous lumisection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09ecf5c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SHOW_FIGS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SHOW_FIGS' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "doPlotting = DO_PLOTTING\n",
    "saveFigs = SAVE_FIGS\n",
    "showFigs = SHOW_FIGS\n",
    "verbose = 1\n",
    "imageDir = f\"images/{DIR_NAME}\"\n",
    "if saveFigs: \n",
    "    #Ensure the saving directory exists\n",
    "    if not os.path.exists(f\"images/{DIR_NAME}\"): os.makedirs(f\"images/{DIR_NAME}\")\n",
    "    print(f\"Images will be saved in: {imageDir}\")\n",
    "#Create lists for tracking the anomalous lumisections in all of the runs\n",
    "# all_anomalous_runs\n",
    "# all_anomalous_lumisections\n",
    "# all_anomalous_lumisections_unique\n",
    "# all_anomalous_powergroups\n",
    "# total_anomalous_lumisections = len(all_anomalous_lumisections_unique) Either one works\n",
    "total_anomalous_lumisections = num_anomalous_lumisections\n",
    "#Inspect the original data, the prediction, the loss, and the binary loss\n",
    "print(f\"There are {total_anomalous_lumisections} anomalous lumisections from run {all_runs[0]} to run {all_runs[-1]}!\")\n",
    "#print(f\"The anomalous powergroups are:\\n{anomalous_powergroups}\\n\")\n",
    "counter = 0 #Counter for keeping track of if we're almost done\n",
    "previous_counter = 0 #Just used to make the print statements cleaner. Only print progress if the data_dict actually had an anomalous lumisection\n",
    "for i, data_dict in enumerate(data_dict_list):\n",
    "    startTime = time.time()\n",
    "    #Extract the relevant information from the data_dict\n",
    "    run_number = data_dict['run_number']\n",
    "    lumisections = data_dict['lumisections']\n",
    "    multi_lumi_data = data_dict['data_cross']\n",
    "    mes_pred_cross = data_dict['predictions_cross']\n",
    "    losses_cross = data_dict['losses']\n",
    "    losses_binary_cross = data_dict['losses_binary']\n",
    "    anomalous_lumisections = data_dict['anomalous_lumisections']\n",
    "    anomalous_powergroups = data_dict['anomalous_powergroups']\n",
    "    \n",
    "    previous_lumisection = -1\n",
    "    for j, lumisection in enumerate(anomalous_lumisections):\n",
    "        counter += 1 #Increase the counter for each anomalous lumisection plotted\n",
    "        index = lumiToIndex(lumisections, lumisection)\n",
    "        data = multi_lumi_data[index]\n",
    "        prediction = mes_pred_cross[index]\n",
    "        plot_losses = losses_cross[index]\n",
    "        plot_losses_binary = losses_binary_cross[index]\n",
    "        anomalous_powergroup = anomalous_powergroups[j]\n",
    "\n",
    "        if verbose>1: print(f\"Anomalous Power Group: {anomalous_powergroup} \\t in Lumisection: {lumisection} \\t in Run: {run_number}\")\n",
    "        #Plotting and determining whether the current lumisection is the same as the previous one due to multi disk anomalies\n",
    "        if doPlotting and lumisection != previous_lumisection: #only print if the current lumisection is not the previous one again\n",
    "            testingFig, testingAxes = plot_testing_plots(data, prediction, plot_losses, plot_losses_binary, run_number, lumisection, ring_num, \n",
    "                                                         directory=imageDir, saveFig=saveFigs, showFig=showFigs)\n",
    "            #plt.show()\n",
    "            if verbose>0 and showFigs: \n",
    "                print(f\"The Anomalous Powergroup in the Above Image is: {anomalous_powergroup}\")\n",
    "        elif doPlotting and lumisection == previous_lumisection:\n",
    "            if verbose>0 and showFigs: print(f\"Due to a Multi-Disk Anomaly, the Other Anomalous Powergroup is: {anomalous_powergroup}\")\n",
    "        \n",
    "        previous_lumisection = lumisection\n",
    "    #If verbose and there was an anomalous lumisection in that run\n",
    "    if verbose>0 and counter != previous_counter: print(f\"Done Run {run_number} and {counter}/{total_anomalous_lumisections} Anomalous Lumisections in {time.time()-startTime} Seconds!\")\n",
    "    previous_counter = counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda7c99c",
   "metadata": {},
   "source": [
    "## Extra: Plot Specific Data, Predictions, Losses, and Binary Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80c954c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#Inspect the original data, the prediction, the loss, and the binary loss\n",
    "run_number = 393240\n",
    "lumi_number = 3083\n",
    "verbose = 1\n",
    "save_fig = False\n",
    "\n",
    "#Find the data_dict with a run number of run_number and extract the relevant information\n",
    "#https://stackoverflow.com/questions/8653516/search-a-list-of-dictionaries-in-python. See here\n",
    "plot_dict = next((data_dict for data_dict in data_dict_list if data_dict[\"run_number\"] == run_number), None)\n",
    "if plot_dict == None:\n",
    "    raise Exception(f\"Run Number {run_number} is not in the list of dictionaries!\")\n",
    "\n",
    "#Extract the relavant information for plotting\n",
    "lumisections = plot_dict['lumisections']\n",
    "if verbose>1: print(f\"There are {len(lumisections)} Available Lumisections:\\n{lumisections}\")\n",
    "multi_lumi_data = plot_dict['data_cross']\n",
    "mes_pred_cross = plot_dict['predictions_cross']\n",
    "losses_cross = plot_dict['losses']\n",
    "losses_binary_cross = plot_dict['losses_binary']\n",
    "\n",
    "#Extract the anomalous powergroups in that lumisection\n",
    "anomalous_lumisections = plot_dict['anomalous_lumisections'] #anomalous_lumisections is a comprehensive list of anomalous lumisections, \n",
    "#Containing duplicates for lumisections with multiple anomalous powergroups\n",
    "desired_lumi_indices = np.where(anomalous_lumisections == lumi_number)[0] #np.where needs this extra [0]\n",
    "anomalous_powergroups = plot_dict['anomalous_powergroups']\n",
    "desired_anomalous_powergroups = anomalous_powergroups[desired_lumi_indices]\n",
    "\n",
    "\n",
    "#Extract the specific lumisection data\n",
    "index = lumiToIndex(lumisections, lumi_number)\n",
    "data = multi_lumi_data[index]\n",
    "prediction = mes_pred_cross[index]\n",
    "plot_losses = losses_cross[index]\n",
    "plot_losses_binary = losses_binary_cross[index]\n",
    "if verbose>0: print(f\"All Anomalous Powergroups in This Lumisection: {desired_anomalous_powergroups}\")\n",
    "\n",
    "\n",
    "print(f\"Run Number: {run_number}\\tLumi Number: {lumi_number}\")\n",
    "testingFig, testingAxes = plot_testing_plots(data, prediction, plot_losses, \n",
    "                                             plot_losses_binary, run_number, lumi_number, ring_num, \n",
    "                                             saveFig=save_fig, showFig=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf68fde4",
   "metadata": {},
   "source": [
    "## Extra: Inspect Specific Data Dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7650905",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%skip True\n",
    "run_number = 393240\n",
    "verbose = 1\n",
    "show_fig = True\n",
    "save_fig = True\n",
    "save_directory = 'images'\n",
    "\n",
    "inspect_dict = next((data_dict for data_dict in data_dict_list if data_dict[\"run_number\"] == run_number), None)\n",
    "\n",
    "print(inspect_dict[\"run_number\"])\n",
    "print(inspect_dict[\"lumisections\"])\n",
    "print(inspect_dict[\"data\"].shape)\n",
    "\n",
    "#Extract the relavant information for plotting\n",
    "lumisections = inspect_dict['lumisections']\n",
    "if verbose>1: print(f\"There are {len(lumisections)} Available Lumisections:\\n{lumisections}\")\n",
    "multi_lumi_data = inspect_dict['data_cross']\n",
    "mes_pred_cross = inspect_dict['predictions_cross']\n",
    "losses_cross = inspect_dict['losses']\n",
    "losses_binary_cross = inspect_dict['losses_binary']\n",
    "\n",
    "#Extract the anomalous powergroups in that lumisection\n",
    "anomalous_lumisections = inspect_dict['anomalous_lumisections'] #anomalous_lumisections is a comprehensive list of anomalous lumisections, \n",
    "#Containing duplicates for lumisections with multiple anomalous powergroups\n",
    "\n",
    "#Save all of the images for a specific length of lumisections and for a specific run for analyzing results more in-depth\n",
    "#Useful for finding anomalies in training data that may not have been found\n",
    "lumi_start = 3083\n",
    "lumi_end = 3494\n",
    "lumi_range = np.arange(lumi_start, lumi_end+1, 1) #Set up the range of lumis to plot\n",
    "#Exclude lumis where we already know there is an anomaly and they have already been discarded from training\n",
    "excluded_lumis = np.array([168, 169, 523, 524, 525, 526, 573, 574, 575, 576, 577, 578, 853, 854, 855, 856, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224 ,225 ,226 ,227 ,228 ,229 ,230 ,231 ,232 ,233 ,234 ,235 ,236 ,237 ,238 ])\n",
    "# lumis_to_plot = [i for i in lumi_range if i not in excluded_lumis] #list comprehension way\n",
    "#Numpy way\n",
    "indices=np.argwhere(np.isin(lumi_range,excluded_lumis))\n",
    "lumis_to_plot=np.delete(lumi_range,indices)\n",
    "print(lumis_to_plot.shape)\n",
    "\n",
    "for idx, lumisection in enumerate(lumis_to_plot):\n",
    "    \n",
    "    #Only plot every tenth lumisection\n",
    "    if lumisection % 10 != 0:\n",
    "        continue\n",
    "\n",
    "    desired_lumi_indices = np.where(anomalous_lumisections == lumisection)[0] #np.where needs this extra [0]\n",
    "    anomalous_powergroups = inspect_dict['anomalous_powergroups']\n",
    "    desired_anomalous_powergroups = anomalous_powergroups[desired_lumi_indices]\n",
    "\n",
    "\n",
    "    #Extract the specific lumisection data\n",
    "    try:\n",
    "        index = lumiToIndex(lumisections, lumisection)\n",
    "    except:\n",
    "        print(f\"ERROR: Desired Lumisection {lumisection} not found in lumisection array!\")\n",
    "        continue\n",
    "    data = multi_lumi_data[index]\n",
    "    prediction = mes_pred_cross[index]\n",
    "    plot_losses = losses_cross[index]\n",
    "    plot_losses_binary = losses_binary_cross[index]\n",
    "    if verbose>0: print(f\"All Anomalous Powergroups in This Lumisection: {desired_anomalous_powergroups}\")\n",
    "\n",
    "\n",
    "    print(f\"Run Number: {run_number}\\tLumi Number: {lumisection}\")\n",
    "    testingFig, testingAxes = plot_testing_plots(data, prediction, plot_losses, \n",
    "                                                 plot_losses_binary, run_number, lumisection, ring_num, \n",
    "                                                 saveFig=save_fig, showFig=show_fig, directory=save_directory)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61a2d9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c74ccca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.75 s, sys: 833 ms, total: 6.59 s\n",
      "Wall time: 6.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import numpy as np\n",
    "\n",
    "chunk_size = 2000  # tune depending on memory limits\n",
    "\n",
    "for data_dict in data_dict_list:\n",
    "    data = data_dict[\"data\"].astype(np.float32, copy=False)\n",
    "    preds = data_dict[\"predictions\"].astype(np.float32, copy=False)\n",
    "\n",
    "    # Preallocate output arrays once (same shape as input)\n",
    "    losses = np.empty_like(data, dtype=np.float32)\n",
    "    losses_binary = np.empty_like(data, dtype=bool)\n",
    "\n",
    "    # Process in chunks along the first axis\n",
    "    for i in range(0, data.shape[0], chunk_size):\n",
    "        sl = slice(i, i + chunk_size)\n",
    "\n",
    "        # In-place diff and square\n",
    "        diff = data[sl] - preds[sl]\n",
    "        np.square(diff, out=diff)\n",
    "\n",
    "        # Save results into the preallocated arrays\n",
    "        losses[sl] = diff\n",
    "        losses_binary[sl] = diff > LOSS_THRESHOLD\n",
    "\n",
    "    # Add the crosses back (try to make add_cross in-place if possible)\n",
    "    losses_cross = add_cross(losses)\n",
    "    losses_binary_cross = add_cross(losses_binary)\n",
    "\n",
    "    # Store results\n",
    "    data_dict[\"losses\"] = losses_cross\n",
    "    data_dict[\"losses_binary\"] = losses_binary_cross\n"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
