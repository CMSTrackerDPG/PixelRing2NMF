{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7236284d",
   "metadata": {},
   "source": [
    "# Finding Anomalies for an Entire Era\n",
    "This notebook is for compiling an Excel file detailing all of the anomalies in a single era. \n",
    "\n",
    "Specify the Era\n",
    "\n",
    "1. Load the appropriate model\n",
    "2. Import all runs/lumisections that pass the DCS flags\n",
    "3. Predict on the lumisections\n",
    "4. Loop over each run\n",
    "    * Split the long combined lumisection, data, and predictions arrays with their respective run number\n",
    "    * Store the predictions with the specific run in a dictionary\n",
    "    * Keep a running list of each dictionary\n",
    "5. Loop over each run\n",
    "    * Calculate the losses and binary losses (Do this in separate loop so we can change the loss threshold)\n",
    "6. Loop over each run and analyze the anomalies\n",
    "    * Try to figure out a data storage format that we can run through the normal Excel creation format\n",
    "7. Create an plot for each anomalous lumisection\n",
    "\n",
    "Folders\n",
    "* Folder --> /eos/user/a/alaperto/SWAN_projects/NMFolder/models/\n",
    "* Data by Lyka --> /eos/user/l/llambrec/dialstools-output/\n",
    "* Original Jake --> /eos/user/j/jomorris/SWAN_projects/NMF Testing/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7f692449",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, inspect\n",
    "import importlib\n",
    "import functions\n",
    "importlib.reload(functions)\n",
    "from functions import *\n",
    "\n",
    "called = set()\n",
    "\n",
    "def trace_calls(frame, event, arg):\n",
    "    if event != \"call\":\n",
    "        return\n",
    "    func = frame.f_code\n",
    "    filename = inspect.getsourcefile(func)\n",
    "    if filename and filename.endswith(\"functions.py\"):\n",
    "        called.add(func.co_name)\n",
    "    return trace_calls\n",
    "\n",
    "sys.settrace(trace_calls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a85f80a1",
   "metadata": {
    "init_cell": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.55 ms, sys: 0 ns, total: 1.55 ms\n",
      "Wall time: 1.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# imports\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import joblib\n",
    "import importlib\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nmf2d import NMF2D\n",
    "\n",
    "#import functions\n",
    "#importlib.reload(functions);\n",
    "#from functions import *\n",
    "\n",
    "optimized_powerGroupStringsList = np.array(['FPix_BmO_D3_ROG4','FPix_BmO_D2_ROG4','FPix_BmO_D1_ROG4','FPix_BmO_D3_ROG3','FPix_BmO_D2_ROG3','FPix_BmO_D1_ROG3','FPix_BmO_D3_ROG2','FPix_BmO_D2_ROG2','FPix_BmO_D1_ROG2','FPix_BmO_D3_ROG1','FPix_BmO_D2_ROG1','FPix_BmO_D1_ROG1','FPix_BmI_D3_ROG1','FPix_BmI_D2_ROG1','FPix_BmI_D1_ROG1','FPix_BmI_D3_ROG2','FPix_BmI_D2_ROG2','FPix_BmI_D1_ROG2','FPix_BmI_D3_ROG3','FPix_BmI_D2_ROG3','FPix_BmI_D1_ROG3','FPix_BmI_D3_ROG4','FPix_BmI_D2_ROG4','FPix_BmI_D1_ROG4','FPix_BpO_D1_ROG4','FPix_BpO_D2_ROG4','FPix_BpO_D3_ROG4','FPix_BpO_D1_ROG3','FPix_BpO_D2_ROG3','FPix_BpO_D3_ROG3','FPix_BpO_D1_ROG2','FPix_BpO_D2_ROG2','FPix_BpO_D3_ROG2','FPix_BpO_D1_ROG1','FPix_BpO_D2_ROG1','FPix_BpO_D3_ROG1','FPix_BpI_D1_ROG1','FPix_BpI_D2_ROG1','FPix_BpI_D3_ROG1','FPix_BpI_D1_ROG2','FPix_BpI_D2_ROG2','FPix_BpI_D3_ROG2','FPix_BpI_D1_ROG3','FPix_BpI_D2_ROG3','FPix_BpI_D3_ROG3','FPix_BpI_D1_ROG4','FPix_BpI_D2_ROG4','FPix_BpI_D3_ROG4'])\n",
    "#A list of all of the quarters of the detector\n",
    "QUARTERS = np.array([['FPix_BmI_D3_ROG1','FPix_BmI_D3_ROG2','FPix_BmI_D3_ROG3','FPix_BmI_D3_ROG4','FPix_BmI_D2_ROG1','FPix_BmI_D2_ROG2','FPix_BmI_D2_ROG3','FPix_BmI_D2_ROG4','FPix_BmI_D1_ROG1','FPix_BmI_D1_ROG2','FPix_BmI_D1_ROG3','FPix_BmI_D1_ROG4'], ['FPix_BmO_D3_ROG1','FPix_BmO_D3_ROG2','FPix_BmO_D3_ROG3','FPix_BmO_D3_ROG4','FPix_BmO_D2_ROG1','FPix_BmO_D2_ROG2','FPix_BmO_D2_ROG3','FPix_BmO_D2_ROG4','FPix_BmO_D1_ROG1','FPix_BmO_D1_ROG2','FPix_BmO_D1_ROG3','FPix_BmO_D1_ROG4'], ['FPix_BpI_D1_ROG1','FPix_BpI_D1_ROG2','FPix_BpI_D1_ROG3','FPix_BpI_D1_ROG4','FPix_BpI_D2_ROG1','FPix_BpI_D2_ROG2','FPix_BpI_D2_ROG3','FPix_BpI_D2_ROG4','FPix_BpI_D3_ROG1','FPix_BpI_D3_ROG2','FPix_BpI_D3_ROG3','FPix_BpI_D3_ROG4'], ['FPix_BpO_D1_ROG1','FPix_BpO_D1_ROG2','FPix_BpO_D1_ROG3','FPix_BpO_D1_ROG4','FPix_BpO_D2_ROG1','FPix_BpO_D2_ROG2','FPix_BpO_D2_ROG3','FPix_BpO_D2_ROG4','FPix_BpO_D3_ROG1','FPix_BpO_D3_ROG2','FPix_BpO_D3_ROG3','FPix_BpO_D3_ROG4']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9104f71",
   "metadata": {},
   "source": [
    "|        2024 Era     | C | D | E | E | F |   F  |   G  |   H  |   I  |   I  |\n",
    "|:-------------------:|:-:|:-:|:-:|:-:|:-:|:----:|:----:|:----:|:----:|:----:|\n",
    "|       Version       | 1 | 1 | 1 | 2 | 1 |   1  |   1  |   1  |   1  |   2  |\n",
    "|        Period       | 1 | 1 | 1 | 1 | 1 |   2  |   2  |   2  |   2  |   2  |\n",
    "|    Model Ring 1       | 1 | 1 | 1 | 1 | 1 |   2  |   2  |   2  |   2  |   2  |\n",
    "|    Model Ring 2       | 5 | 5 | 5 | 5 | 5 |   7  |   7  |   7  |   7  |   7  |\n",
    "\n",
    "|        2025 Era     | C | C | C | D | E | F |   G  |   H  |   I  |   J  |\n",
    "|:-------------------:|:-:|:-:|:-:|:-:|:-:|:-:|:----:|:----:|:----:|:----:|\n",
    "|       Version       | 1 | 1 | 2 | 1 | 1 | 1 |   1  |   1  |   1  |   1  |\n",
    "|        Period       | 3 | 4 | 4 | 4 | 4 | X |   X  |   X  |   X  |   X  |\n",
    "|    Model Ring 1       | 3 | 8 | 8 | 8 | 8 | X |   X  |   X  |   X  |   X  |\n",
    "|    Model Ring 2       | 7 | 7 | 7 | 7 | 7 |     |     |     |     |     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9beaec",
   "metadata": {},
   "source": [
    "# Important Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "daead652",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the era: import and search for anomalies\n",
    "RING = 2\n",
    "YEAR, ERA, VERSION, PERIOD = 2024, \"I\", 1, 2\n",
    "\n",
    "#ring 1 --> 1 - 2 - 3 - 4\n",
    "#ring 2 --> 5 - 7 - 7 - 7\n",
    "number, model_period, type = 7, 3, 1\n",
    "model_name = f'model_{number}_PXRing_{RING}_period_{model_period}_type_{type}.pkl'\n",
    "\n",
    "#For both\n",
    "ANOMALY_CUTOFF = 40 #Threshold on fraction of powergroup that is Bad --> Define LS as anomalous\n",
    "\n",
    "# Model thresholds Ring 1\n",
    "if RING == 1:\n",
    "    EDITION = 1\n",
    "    if YEAR == 2024:\n",
    "        LOSS_THRESHOLD = 4e5 #Threshold on Loss of the ROC --> Define ROC as Bad\n",
    "    elif YEAR == 2025:\n",
    "        LOSS_THRESHOLD = 9e5\n",
    "    #EDITION = 4 #Special test for 900 LS, model 8\n",
    "# Model thresholds Ring 2\n",
    "elif RING == 2:\n",
    "    EDITION = 2\n",
    "    LOSS_THRESHOLD = 1e5 #Threshold on Loss of the ROC --> Define ROC as Bad\n",
    "    #EDITION = 3 #Special for splitting in half era 2024G_v1 and 2025D_v1\n",
    "\n",
    "#Plotting Globals\n",
    "DO_PLOTTING = False #Whether or not to plot EVERY anomalous lumisection in this era. WARNING: Takes a long time. ~13mins for 93 anomalies\n",
    "SAVE_FIGS = False #Whether or not to also SAVE the plot of every anomalous lumisection. DO_PLOTTING must also be True for the images to be saved\n",
    "\n",
    "#Some light calculation of important variables\n",
    "file = f'../data/ZeroBias-Run{YEAR}{ERA}-PromptReco-v{VERSION}-DQMIO-PixelPhase1-Phase1_MechanicalView-PXForward-clusters_per_SignedDiskCoord_per_SignedBladePanelCoord_PXRing_{RING}.parquet'\n",
    "oms_json = f'../omsdata/omsdata_Run{YEAR}{ERA}-v{VERSION}.json'\n",
    "#ring_num = int(file[-9]) #The -9th character is ALWAYS the ring number for our data\n",
    "ring_num = RING\n",
    "\n",
    "#The directory name to use for \n",
    "DIR_NAME = f\"../results/output_{YEAR}{ERA}_v{VERSION}_period_{PERIOD}_PXRing_{RING}_edition_{EDITION}\" \n",
    "if not os.path.exists(DIR_NAME): os.makedirs(DIR_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2b5c3d",
   "metadata": {},
   "source": [
    "## 1. Load the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b439243b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Model: ../models/model_7_PXRing_2_period_3_type_1.pkl\n",
      "Model Shape: [136, 48]\n"
     ]
    }
   ],
   "source": [
    "nmf_file = f'../models/{model_name}'\n",
    "nmf = joblib.load(nmf_file)\n",
    "\n",
    "print(f\"Loaded Model: {nmf_file}\")\n",
    "print(f\"Model Shape: {nmf.xshape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6acd06e",
   "metadata": {},
   "source": [
    "## 2. Import all runs/lumisections that pass the DCS flags\n",
    "Import the entire era at once so we only have to go to the disk once. \n",
    "\n",
    "Use the OMS JSON to filter only the lumisections that pass the DCS flags. Helps to reduce unnecessary predictions on bad lumisections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0cd95e8c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 26 Runs and 13941 Lumisections that Pass All DCS Flags: \n",
      " [386478 386505 386508 386509 386553 386554 386592 386593 386594 386604\n",
      " 386605 386614 386615 386616 386617 386618 386629 386630 386640 386642\n",
      " 386661 386668 386672 386673 386679 386693]\n",
      "CPU times: user 10.1 s, sys: 5.68 s, total: 15.7 s\n",
      "Wall time: 23.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Any logic for when parts of the detector are disabled. \n",
    "extra_filters = []\n",
    "#Logic for Era F period 1/2 and 2025 Era Cv1 period 3/4\n",
    "if YEAR == 2024 and ERA == \"F\" and PERIOD == 1:\n",
    "    extra_filters.append(('run_number', '<', 382799))\n",
    "elif YEAR == 2024 and ERA == \"F\" and PERIOD == 2:\n",
    "    extra_filters.append(('run_number', '>=', 382799))\n",
    "elif YEAR == 2024 and ERA == \"G\" and RING == 2 and EDITION == 2: #Split in half 2024G_v1, too heavy\n",
    "    extra_filters.append(('run_number', '<', 384684))\n",
    "elif YEAR == 2024 and ERA == \"G\" and RING == 2 and EDITION == 3: #Split in half 2024G_v1, too heavy\n",
    "    extra_filters.append(('run_number', '>', 384684))\n",
    "elif YEAR == 2025 and ERA == \"C\" and VERSION == 1 and PERIOD == 3:\n",
    "    extra_filters.append(('run_number', '<=', 392668))\n",
    "elif YEAR == 2025 and ERA == \"C\" and VERSION == 1 and PERIOD == 4:\n",
    "    extra_filters.append(('run_number', '>', 392668))\n",
    "elif YEAR == 2025 and ERA == \"C\" and VERSION == 2 and PERIOD == 4:\n",
    "    extra_filters.append(('run_number', '<', 393512))#Machine Development runs\n",
    "elif YEAR == 2025 and ERA == \"D\" and RING == 2 and EDITION == 2: #Split in half 2025D_v1, too heavy\n",
    "    extra_filters.append(('run_number', '<', 395432))\n",
    "elif YEAR == 2025 and ERA == \"D\" and RING == 2 and EDITION == 3: #Split in half 2025D_v1, too heavy\n",
    "    extra_filters.append(('run_number', '>', 395432))\n",
    "\n",
    "#Testing the function to import AND filter an entire era at once\n",
    "multi_lumi_data, all_runs, lumis, df = extract_data_whole_era(file, oms_json, extra_filters=extra_filters)\n",
    "del df\n",
    "all_runs, indices = np.unique(all_runs, return_index=True)\n",
    "indices = indices[1:] #The first index in indices is always 0, since the first number is always unique, so we discard that. \n",
    "print(f\"There are {len(all_runs)} Runs and {len(lumis)} Lumisections that Pass All DCS Flags: \\n\", all_runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943618fb",
   "metadata": {},
   "source": [
    "## 3. Predict on the lumisections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ccbfb5dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions Already Exist!\n",
      "Loading predictions from ../results/output_2024I_v1_period_2_PXRing_2_edition_2/Predictions.npy\n",
      "Shape: (13941, 136, 48)\n",
      "CPU times: user 486 ms, sys: 916 ms, total: 1.4 s\n",
      "Wall time: 1.44 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Predict or not predict (30 mins for 100k LS)\n",
    "FORCE_PREDICT = False\n",
    "\n",
    "#Remove the cross so we can predict on it\n",
    "multi_lumi_data_no_cross = remove_cross(multi_lumi_data, RING)\n",
    "\n",
    "#Save predictions array to a file (since they take so long to produce)\n",
    "pred_filename = f\"{DIR_NAME}/Predictions\"\n",
    "\n",
    "if os.path.exists(pred_filename + '.npy') and FORCE_PREDICT == False:\n",
    "    print(f\"Predictions Already Exist!\")\n",
    "    print(f\"Loading predictions from {pred_filename}.npy\")\n",
    "    mes_pred = np.load(pred_filename + '.npy')\n",
    "    print(f\"Shape: {mes_pred.shape}\")\n",
    "else:\n",
    "    #If we don't already have a prediction directory then make it\n",
    "    if FORCE_PREDICT: \n",
    "        print(f\"FORCE_PREDICT is True. ---> Overwrite file at {pred_filename}.npy\")\n",
    "    else:\n",
    "        print(\"Predictions don't exist yet! Starting Prediction. \")\n",
    "\n",
    "    #Predict on the data\n",
    "    print(f'Predicting...')\n",
    "    start_time = time.time()\n",
    "    mes_pred = nmf.predict(multi_lumi_data_no_cross)\n",
    "    np.save(pred_filename, mes_pred)\n",
    "    print(f\"Done Predicting in {time.time() - start_time} Seconds!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8d5473",
   "metadata": {},
   "source": [
    "## 4. Loop over each run\n",
    "    * Split the long combined lumisection, data, and predictions arrays with their respective run number\n",
    "    * Store the predictions with the specific run in a dictionary\n",
    "    * Keep a running list of each dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3cff02fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.6 ms, sys: 7.94 ms, total: 20.5 ms\n",
      "Wall time: 40.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Verbose=0: No prints. Verbose=1: Some prints. Verbose>=2: Some very long prints\n",
    "verbose = 0\n",
    "\n",
    "#Split the long combined arrays of the lumisections, the data, and the predictions\n",
    "lumisections = np.split(lumis, indices)\n",
    "data_arr = np.split(multi_lumi_data_no_cross, indices)\n",
    "pred_arr = np.split(mes_pred, indices)\n",
    "\n",
    "#print(pred_arr[1].shape)\n",
    "\n",
    "#Now our data is splitted into arrays by run number, we can loop over the run numbers, generate our data_dicts and put them in a data_dict_list.\n",
    "#So it is in a format ready to be used to calculate the losses and anomalies. \n",
    "#Create a list to store all of the lumisections and predictions\n",
    "data_dict_list = list(np.empty_like(all_runs)) \n",
    "for index, run_number in enumerate(all_runs):\n",
    "    data_dict = {}\n",
    "    data_dict[\"run_number\"] = run_number\n",
    "    data_dict[\"lumisections\"] = lumisections[index]\n",
    "    data_dict[\"data\"] = data_arr[index]\n",
    "    data_dict[\"predictions\"] = pred_arr[index]\n",
    "    #Add the cross into the predictions and data\n",
    "    #data_dict[\"data_cross\"] = add_cross(data_arr[index])\n",
    "    #data_dict[\"predictions_cross\"] = add_cross(pred_arr[index])\n",
    "    \n",
    "    #Add this run to the data dict list\n",
    "    data_dict_list[index] = data_dict    \n",
    "    #Printing\n",
    "    if verbose>0:\n",
    "        print(f\"Run Number: {run_number}\")\n",
    "        print(f\"\\tThere are {len(lumisections[index])} Extracted Lumisections:\\n\\t{lumisections[index]}\\n\")\n",
    "    if verbose>1:\n",
    "        print(data_dict)\n",
    "        \n",
    "if verbose>0:\n",
    "    print(data_dict_list[0])\n",
    "elif verbose>1:\n",
    "    print(data_dict_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f1d220",
   "metadata": {},
   "source": [
    "## 5. Loop over each run\n",
    "    * Calculate the losses and binary losses (Do this in separate loop so we can change the loss threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6a181376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.29 s, sys: 10.3 ms, total: 4.3 s\n",
      "Wall time: 4.36 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#Loop over each data dictionary in the list and calculate the losses and binary losses. \n",
    "#Add these to the dictionaries as we go along. \n",
    "for index, data_dict in enumerate(data_dict_list):\n",
    "    #Extract the needed info from the data_dict\n",
    "    multi_lumi_data_no_cross = data_dict[\"data\"]\n",
    "    mes_pred = data_dict[\"predictions\"]\n",
    "    \n",
    "    #Calculate losses\n",
    "    losses = np.square(multi_lumi_data_no_cross - mes_pred)\n",
    "    losses_binary = (losses > LOSS_THRESHOLD).astype(int)\n",
    "    \n",
    "    #Add the crosses back\n",
    "    #losses_cross = add_cross(losses)\n",
    "    losses_binary_cross = add_cross(losses_binary)\n",
    "    \n",
    "    #Add new entries to the data_dict\n",
    "    #This will automatically update the dictionaries in the data_dict_list\n",
    "    #data_dict[\"losses\"] = losses_cross\n",
    "    data_dict[\"losses_binary\"] = losses_binary_cross"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb061d02",
   "metadata": {},
   "source": [
    "## 6. Loop over each run and analyze the anomalies. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d315c89b",
   "metadata": {},
   "source": [
    "### Analyze Each Lumisection of Each Run for Anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "63879827",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming results...\n",
      "Era 2024I_v1 (Run 386478 to Run 386693)\n",
      "CPU times: user 2min 34s, sys: 1min 18s, total: 3min 52s\n",
      "Wall time: 8min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "testingtime = False\n",
    "verbose = False\n",
    "\n",
    "#Create lists for tracking the anomalous lumisections in all of the runs\n",
    "all_anomalous_runs = []\n",
    "all_anomalous_lumisections = []\n",
    "all_anomalous_powergroups = []\n",
    "\n",
    "for i, data_dict in enumerate(data_dict_list):\n",
    "    #Extract the required info from the data_dict\n",
    "    run_number = data_dict[\"run_number\"]\n",
    "    lumisections = data_dict[\"lumisections\"]\n",
    "    losses_binary_cross = data_dict[\"losses_binary\"]\n",
    "    \n",
    "    #Currently a list of anomalous lumisections and their powergroups\n",
    "    #These arrays are prevented from getting out of sync by still appending \n",
    "    #the anomalous lumisection even if that lumisection was already marked bad with a different powergroup\n",
    "    anomalous_lumisections = []\n",
    "    anomalous_powergroups = []\n",
    "\n",
    "    for index, lumisection in enumerate(lumisections):\n",
    "        if verbose: print(f\"Index: {index} \\t Lumisection: {lumisection}\")\n",
    "\n",
    "        for j, powergroup in enumerate(optimized_powerGroupStringsList):\n",
    "            #if not testingtime: print(f\"Power Group String: {powergroup}\")\n",
    "            powerGroupSlice, diskSlice = powerGroupToIndex(powergroup, RING)\n",
    "\n",
    "            #Access each power group in each lumisection and see if more than 40% of the bins are on\n",
    "            #A bit ugly, but this was the fastest way I found. Saved about .1 seconds over saving the powergroup data to another variable. \n",
    "            if int(np.sum(losses_binary_cross[index, powerGroupSlice, diskSlice].flatten())) >= int(ANOMALY_CUTOFF/100 * losses_binary_cross[index, powerGroupSlice, diskSlice].flatten().size):\n",
    "                if verbose: print(f\"Anomalous Power Group: {powergroup} \\t in Lumisection: {lumisection} \\t with Binary Sum: {np.sum(powerGroup_data)}\")\n",
    "                all_anomalous_runs.append(run_number)\n",
    "                anomalous_lumisections.append(lumisection)\n",
    "                anomalous_powergroups.append(powergroup)\n",
    "                \n",
    "            #This is used to pull out specific lumisection and check their binary loss occupancy\n",
    "            #if run_number == 379660 and lumisection == 291:\n",
    "            #    print(f\"Powergroup: {powergroup}\")\n",
    "            #    print(f\"Powergroup Size:{losses_binary_cross[index, powerGroupSlice, diskSlice].flatten().size}\")\n",
    "            #    print(f\"Sum of Binary Loss:{np.sum(losses_binary_cross[index, powerGroupSlice, diskSlice].flatten())}\\n\")\n",
    "            #    save_digis_png(losses_binary_cross[index], run_number, lumisection, RING)\n",
    "                \n",
    "    #Update the data_dict with the anomalous lumisections and powergroups\n",
    "    #I don't have a plan for them currently, but it could be useful\n",
    "    data_dict[\"anomalous_lumisections\"] = np.array(anomalous_lumisections)\n",
    "    data_dict[\"anomalous_powergroups\"] = np.array(anomalous_powergroups)\n",
    "    \n",
    "    #EXTEND the anomalous lumisections and powegroups to the ALL list. Extend keeps the list flat\n",
    "    all_anomalous_lumisections.extend(anomalous_lumisections)\n",
    "    all_anomalous_powergroups.extend(anomalous_powergroups)\n",
    "    \n",
    "# print(all_anomalous_runs)\n",
    "# print(all_anomalous_lumisections)\n",
    "# print(all_anomalous_powergroups)\n",
    "# print(data_dict_list[-2])\n",
    "#num_anomalous_lumisections, all_anomalous_lumisections_unique = calcNumAnomalousLumisections(data_dict_list)\n",
    "print(f'Resuming results...')\n",
    "print(f\"Era {YEAR}{ERA}_v{VERSION} (Run {all_runs[0]} to Run {all_runs[-1]})\")\n",
    "#print(f\"{num_anomalous_lumisections} Anomalous Lumisections\")\n",
    "#For some reason the above number can disagree with the sum of the Num_LS column in Excel. But they are never far off.\n",
    "#Might happen due to lumisections appearing twice in the excel file. Like when a single lumisection has both a single disk anomaly and a multi disk anomaly\n",
    "#The lumisection could be double counted. There may be more cases where this happens. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0a5e62",
   "metadata": {},
   "source": [
    "## Identify Anomaly Types\n",
    "This section will identify runs in lumisections that stay the same then compare the powergroups and see if there are any multi-disk anomalies. \n",
    "\n",
    "If there are no repeating lumisections, then it is just a single disk anomaly. \n",
    "\n",
    "### Create a file summarizing all lumisections and their anomalies\n",
    "#### Run_Number    Lumisection    PRT    Disk    Ring_Num    Anomaly_Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fdb779d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18.1 s, sys: 11 s, total: 29.1 s\n",
      "Wall time: 1min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Identify multi disk anomalies\n",
    "verbose = 0\n",
    "#Create a pandas dataframe that we can use to track EACH anomalous lumisection in each run\n",
    "headers = [\"Run_Number\", \"Lumisection\", \"Powergroup\", \"Disk\", \"Ring_Num\", \"Anomaly_Type\"]\n",
    "all_detailed_anomaly_df = pd.DataFrame(columns=headers)\n",
    "\n",
    "#Loop over each data dict in the data dict list\n",
    "for index, data_dict in enumerate(data_dict_list):\n",
    "    #Extract the relavant information\n",
    "    run_number = data_dict[\"run_number\"]\n",
    "    anomalous_lumisections = data_dict[\"anomalous_lumisections\"]\n",
    "    anomalous_powergroups = data_dict[\"anomalous_powergroups\"]\n",
    "    \n",
    "    #If there are no anomaous lumisections or powergroups then stop this iteration\n",
    "    if anomalous_lumisections.size == 0 or anomalous_powergroups.size == 0:\n",
    "        continue\n",
    "    \n",
    "\n",
    "    #Create dataframe of anomalous lumisections and powergroups\n",
    "    anomaly_df = pd.DataFrame({\"lumisections\": anomalous_lumisections, \"powergroups\": anomalous_powergroups})\n",
    "\n",
    "    if verbose>0: print(\"Anomaly Dataframe: \\n\", anomaly_df, '\\n')\n",
    "\n",
    "    #Create unique arrays to pare down duplicate data\n",
    "    anomalous_lumisections_unique = np.unique(anomalous_lumisections)\n",
    "\n",
    "    #Create a list of dictionaries for easier saving to text\n",
    "    dictList = np.empty_like(anomalous_lumisections_unique, dtype=dict)\n",
    "\n",
    "    detailed_anomaly_df = pd.DataFrame(columns=headers)\n",
    "\n",
    "    # print(\"AHHHHHHH\\n\", detailed_anomaly_df)\n",
    "    if verbose>0: print('-----------------------------------------')\n",
    "    #Loop over each unique lumisection\n",
    "    for index, lumisection in enumerate(anomalous_lumisections_unique):\n",
    "        #These values are the same for single/multi disk anomalies\n",
    "        dataDict = dict.fromkeys(headers)\n",
    "        dataDict[\"Run_Number\"] = run_number\n",
    "        dataDict[\"Lumisection\"] = lumisection\n",
    "        dataDict[\"Ring_Num\"] = ring_num\n",
    "\n",
    "        #Get the lumisection and all of the anomaly powergroups\n",
    "        #If there is only one powergroup, mark it Single Disk, preparing the detailed Pandas anomaly dataframe, then move on\n",
    "        #If there is multiple powergroups, iterate through each pair, breaking on the first Multi-disk anomaly after preparing the detailed Pandas anomaly dataframe\n",
    "        #If there is no Multi-disk anomaly despite there being multiple anomalies in one lumisection, prepare the detailed Pandas anomaly dataframe with EACH anomaly\n",
    "\n",
    "\n",
    "        dataframe = anomaly_df[anomaly_df[\"lumisections\"] == lumisection]\n",
    "        if verbose>0: print(dataframe, '\\n')\n",
    "        powergroups = dataframe[\"powergroups\"].to_list()\n",
    "        if verbose>0: print(f\"Powergroups: {powergroups}\")\n",
    "\n",
    "        #If there are 12 anomalous powergroups in one lumisection, check if it's a whole quarter out\n",
    "        if len(powergroups) == 12:\n",
    "            for quarter in QUARTERS:\n",
    "                #If all of the powergroups are in one quarter, then we can save and break early\n",
    "                if np.all(np.isin(powergroups, quarter)):\n",
    "                    #Fill in the data dict\n",
    "                    m_or_p, I_or_O, disk_number, part_number = analyzePowerGroupString(powergroups[0])\n",
    "                    dataDict[\"Powergroup\"] = ':'.join(powergroups) #Make a string of each powergroup separated by colons. A char not typically used in csv's. \n",
    "                    dataDict[\"Disk\"] = \"-1:-2:-3\" if disk_number < 0 else \"1:2:3\"\n",
    "                    #dataDict[\"Anomaly_Type\"] = \"Whole Quarter\"\n",
    "                    dataDict[\"Anomaly_Type\"] = \"Multi-Disk\"\n",
    "                    dataFrame = pd.Series(dataDict).to_frame().T\n",
    "                    #print(dataFrame)\n",
    "                    detailed_anomaly_df = pd.concat([detailed_anomaly_df, dataFrame])\n",
    "                    #break out of this for loop for the quarters\n",
    "                    break\n",
    "            #then continue to the next unique anomaly\n",
    "            continue\n",
    "\n",
    "        #If there is only one powergroup, mark it as a Single disk anomaly\n",
    "        if len(powergroups) == 1:\n",
    "            #Fill in the data dict\n",
    "            m_or_p, I_or_O, disk_number, part_number = analyzePowerGroupString(powergroups[0])\n",
    "            dataDict[\"Powergroup\"] = powergroups[0]\n",
    "            dataDict[\"Disk\"] = disk_number\n",
    "            dataDict[\"Anomaly_Type\"] = \"Single-Disk\"\n",
    "            #Convert the data dict to a pandas dataframe and concat it to the detailed data frame \n",
    "            #(NOTE: If there are many lumisections, it is technically more effificient to create a list of these dataDicts and concat those all at once)\n",
    "            if verbose>1: print(\"DATA DICT:\", dataDict)\n",
    "            dataFrame = pd.Series(dataDict).to_frame().T\n",
    "            #print(dataFrame)\n",
    "            detailed_anomaly_df = pd.concat([detailed_anomaly_df, dataFrame])\n",
    "            #continue to the next loop\n",
    "            continue\n",
    "\n",
    "        #If there is more than one anomalous powergroup in that lumisection and it's NOT the whole quarter out\n",
    "        all_powergroup_combos = itertools.combinations(powergroups, 2)\n",
    "        #Loop over all pairs of powergroups and search for multi-disk anomalies\n",
    "        dataDictList = [] #Create a list to store all of the possible Single Disk anomalies in case there are multiple anomalies but no Multi Disk anomaly\n",
    "        for powergroup_combo in all_powergroup_combos:\n",
    "            #print(\"ADFJNDKFN\", powergroup_combo)\n",
    "            anomaly_type = powerGroupsToAnomalyType(powergroup_combo[0], powergroup_combo[1])\n",
    "            #Extract relevant information\n",
    "            m_or_p_one, I_or_O_one, disk_number_one, part_number_one = analyzePowerGroupString(powergroup_combo[0])\n",
    "            m_or_p_two, I_or_O_two, disk_number_two, part_number_two = analyzePowerGroupString(powergroup_combo[1])\n",
    "            if anomaly_type == \"Multi-Disk\":\n",
    "                #Fill in the data dict\n",
    "                dataDict[\"Powergroup\"] = powergroup_combo[0] + ':' + powergroup_combo[1]\n",
    "                dataDict[\"Disk\"] = str(disk_number_one) + ':' + str(disk_number_two)\n",
    "                dataDict[\"Anomaly_Type\"] = \"Multi-Disk\"\n",
    "                #Convert the data dict to a pandas dataframe and concat it to the detailed data frame \n",
    "                #(NOTE: If there are many lumisections, it is technically more effificient to create a list of these dataDicts and concat those all at once)\n",
    "                if verbose>1: print(\"DATA DICT:\", dataDict)\n",
    "                dataFrame = pd.Series(dataDict).to_frame().T\n",
    "                #print(dataFrame)\n",
    "                detailed_anomaly_df = pd.concat([detailed_anomaly_df, dataFrame])\n",
    "                #break\n",
    "            else:\n",
    "                #Fill in a data dict for each anomaly\n",
    "                dataDict[\"Powergroup\"] = powergroup_combo[0]\n",
    "                dataDict[\"Disk\"] = str(disk_number_one)\n",
    "                dataDict[\"Anomaly_Type\"] = \"Single-Disk\"\n",
    "                #Convert the data dict to a pandas dataframe and concat it to the detailed data frame \n",
    "                #(NOTE: If there are many lumisections, it is technically more effificient to create a list of these dataDicts and concat those all at once)\n",
    "                if verbose>1: print(\"DATA DICT ONE:\", dataDict)\n",
    "                dataFrame = pd.Series(dataDict).to_frame().T\n",
    "                if verbose>1: print(dataFrame)\n",
    "                detailed_anomaly_df = pd.concat([detailed_anomaly_df, dataFrame])\n",
    "\n",
    "                #Fill in second data dict\n",
    "                dataDict[\"Powergroup\"] = powergroup_combo[1]\n",
    "                dataDict[\"Disk\"] = str(disk_number_two)\n",
    "                dataDict[\"Anomaly_Type\"] = \"Single-Disk\"\n",
    "                #Convert the data dict to a pandas dataframe and concat it to the detailed data frame \n",
    "                #(NOTE: If there are many lumisections, it is technically more effificient to create a list of these dataDicts and concat those all at once)\n",
    "                if verbose>1: print(\"DATA DICT TWO:\", dataDict)\n",
    "                dataFrame = pd.Series(dataDict).to_frame().T\n",
    "                #print(dataFrame)\n",
    "                detailed_anomaly_df = pd.concat([detailed_anomaly_df, dataFrame])\n",
    "            if verbose>0: print('----------------------------------------')\n",
    "        if verbose>0: print('-----------------------------------------')\n",
    "\n",
    "    #Remove all exact duplicate rows\n",
    "    detailed_anomaly_df = detailed_anomaly_df.drop_duplicates()\n",
    "    if verbose>0: print('\\n\\n')\n",
    "    #print(detailed_anomaly_df)\n",
    "    all_detailed_anomaly_df = pd.concat([all_detailed_anomaly_df, detailed_anomaly_df])\n",
    "    #ensure the directory is created\n",
    "    #detailed_anomaly_df.to_excel(f\"{DIR_NAME}/Raw_PXRing_{RING}_{YEAR}{ERA}_Run_{run_number}.xlsx\", index=False, engine='openpyxl')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "67f2c396",
   "metadata": {},
   "source": [
    "## Here the first output (Raw excel)\n",
    "Includes already Single-Multi Disk\n",
    "\n",
    "396398\t414\tFPix_BpI_D1_ROG2\t1\t2\tSingle-Disk\n",
    "\n",
    "396398\t1036\tFPix_BpI_D1_ROG1:FPix_BpI_D2_ROG1\t1:2\t2\tMulti-Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1174f268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Used functions: {'add_cross', 'remove_cross', 'powerGroupToIndex', 'powerGroupsToAnomalyType', '<dictcomp>', 'analyzePowerGroupString', '<listcomp>', 'powerGroupToDiskPanels', 'extract_data_whole_era', 'panelDiskToIndex'}\n",
      "ðŸš« Unused functions: {'condense_powergroup_overlap', 'calcNumAnomalousLumisections', 'condense_lumisection_runs'}\n"
     ]
    }
   ],
   "source": [
    "sys.settrace(None)\n",
    "import inspect\n",
    "\n",
    "# Get all functions defined in my_lib\n",
    "lib_funcs = {\n",
    "    name for name, obj in inspect.getmembers(functions, inspect.isfunction)\n",
    "    if inspect.getsourcefile(obj).endswith(\"functions.py\")\n",
    "}\n",
    "\n",
    "print(\"âœ… Used functions:\", called)\n",
    "print(\"ðŸš« Unused functions:\", lib_funcs - called)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b327c788",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
