{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7215116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import external modules\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7fa0b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import parquet and 2 functions\n",
    "\n",
    "import pyarrow as pa\n",
    "from pyarrow.parquet import ParquetFile, ParquetDataset\n",
    "\n",
    "def read_parquet(path, verbose=False, \n",
    "                 columns=None, batch_size=None, first_batch=None, last_batch=None, batch_ids=None):\n",
    "    if batch_size is None:\n",
    "        # standard case where all rows are read\n",
    "        df = pd.read_parquet(path, columns=columns)\n",
    "    else:\n",
    "        # more involved case where only a section of rows is read\n",
    "        pf = ParquetFile(path)\n",
    "        if verbose:\n",
    "            print('Found following parquet metadata:')\n",
    "            print(pf.metadata)\n",
    "            \n",
    "        # check if contradictory arguments were provided\n",
    "        if batch_ids is not None:\n",
    "            if first_batch is not None or last_batch is not None:\n",
    "                if verbose:\n",
    "                    msg = 'WARNING in read_parquet: cannot provide both batch_ids and first_batch / last_batch;'\n",
    "                    msg += ' first_batch and last_batch will be ignored.'\n",
    "                    print(msg)\n",
    "        else:\n",
    "            if first_batch is None or last_batch is None:\n",
    "                msg = 'ERROR in read_parquet: in batched mode, either batch_ids, or first_batch and last_batch'\n",
    "                msg += ' must be provided; returning None.'\n",
    "                print(msg)\n",
    "                return None\n",
    "            if last_batch < first_batch:\n",
    "                last_batch = first_batch\n",
    "                if verbose:\n",
    "                    msg = f'WARNING in read_parquet: setting last_batch to {last_batch}'\n",
    "                    msg += ' as values smaller than first_batch are not supported.'\n",
    "                    print(msg)\n",
    "            batch_ids = list(range(first_batch, last_batch+1))\n",
    "        \n",
    "        # check available rows and batches\n",
    "        num_rows = pf.metadata.num_rows\n",
    "        num_batches = int((num_rows-1)/batch_size)+1\n",
    "        if max(batch_ids) >= num_batches:\n",
    "            if verbose:\n",
    "                msg = f'WARNING in read_parquet: batch indices greater than {num_batches-1} will be ignored.'\n",
    "                print(msg)\n",
    "        \n",
    "        # iterate through the batches\n",
    "        iterobj = pf.iter_batches(batch_size = batch_size)\n",
    "        batches = []\n",
    "        for batch_idx in range(num_batches):\n",
    "            batch = next(iterobj)\n",
    "            if batch_idx in batch_ids: batches.append(batch)\n",
    "        df = pa.Table.from_batches(batches).to_pandas()\n",
    "        \n",
    "    if verbose:\n",
    "        print(f'Read dataframe with {len(df)} rows and {len(df.columns)} columns.')\n",
    "    return df\n",
    "\n",
    "def get_mes(df, datacolumn='data', xbinscolumn='xbins', ybinscolumn='ybins',\n",
    "            runcolumn='run', lumicolumn='lumi',\n",
    "            runs=None, lumis=None):\n",
    "    if runs is not None: df = select_runs(df, runs, runcolumn=runcolumn)\n",
    "    if lumis is not None: df = select_ls(df, lumis, lumicolumn=lumicolumn)\n",
    "    xbins = int(df[xbinscolumn].values[0])\n",
    "    ybins = int(df[ybinscolumn].values[0])\n",
    "    # note: df['data'][idx] yields an array of 1d arrays;\n",
    "    # need to convert it to a 2d array with np.stack\n",
    "    mes = np.array([np.stack(df[datacolumn].values[i]).reshape(ybins,xbins) for i in range(len(df))])\n",
    "    runs = df[runcolumn].values\n",
    "    lumis = df[lumicolumn].values\n",
    "    return (mes, runs, lumis)\n",
    "\n",
    "def find_oms_indices(runs, lumis, omsjson,\n",
    "                     run_key='run_number', lumi_key='lumisection_number',\n",
    "                     verbose=True):\n",
    "    # check if run_key and lumi_key are present in the omsjson\n",
    "    if not run_key in omsjson.keys():\n",
    "        msg = 'Run key \"{}\" not found in provided omsjson.'.format(run_key)\n",
    "        msg += ' Available keys are: {}'.format(omsjson.keys())\n",
    "        raise Exception(msg)\n",
    "    if not lumi_key in omsjson.keys():\n",
    "        msg = 'Lumi key \"{}\" not found in provided omsjson.'.format(lumi_key)\n",
    "        msg += ' Available keys are: {}'.format(omsjson.keys())\n",
    "        raise Exception(msg)\n",
    "    # parse runs and lumis\n",
    "    runs = np.array(runs).astype(int)\n",
    "    lumis = np.array(lumis).astype(int)\n",
    "    if len(runs.shape)!=1:\n",
    "        raise Exception(f'Provided run numbers array has unexpected shape: {runs.shape}')\n",
    "    if len(lumis.shape)!=1:\n",
    "        raise Exception(f'Provided lumisection numbers array has unexpected shape: {lumis.shape}')\n",
    "    # make sure provided oms runs and lumis are in the same data format\n",
    "    omsruns = np.array(omsjson[run_key]).astype(int)\n",
    "    omslumis = np.array(omsjson[lumi_key]).astype(int)\n",
    "    # combine run and lumisection number into a single unique number\n",
    "    idfactor = 10000 # warning: do not use 1e4 instead of 10000 to avoid conversion from int to float\n",
    "    ids = runs*idfactor + lumis\n",
    "    ids = ids.astype(int)\n",
    "    omsids = omsruns*idfactor + omslumis\n",
    "    omsids = omsids.astype(int)\n",
    "    # check if all ids are in omsids\n",
    "    # note: reduce from error to warning,\n",
    "    # since it seems some lumisections are intrinsically missing in OMS,\n",
    "    # e.g. run 380147, LS 186.\n",
    "    threshold = None\n",
    "    if np.any(np.isin(ids, omsids, invert=True)):\n",
    "        missing_ids_inds = np.isin(ids, omsids, invert=True).nonzero()[0]\n",
    "        missing_ids = ids[missing_ids_inds]\n",
    "        if verbose:\n",
    "            msg = 'WARNING: not all provided lumisections could be found in the oms data.'\n",
    "            msg += f' Missing lumisections are: {missing_ids}'\n",
    "            msg += f' ({len(missing_ids)} / {len(ids)})'\n",
    "            print(msg)\n",
    "        # temporarily add the missing ids to omsids\n",
    "        # (corresponding indices will be set to -1 later)\n",
    "        threshold = len(omsids)\n",
    "        omsids = np.concatenate((omsids, missing_ids))\n",
    "    # find indices of ids in omsids\n",
    "    omsids_sorted_inds = np.argsort(omsids)\n",
    "    omsids_sorted = omsids[omsids_sorted_inds]\n",
    "    indices = np.searchsorted(omsids_sorted, ids, side='left')\n",
    "    indices = omsids_sorted_inds[indices]\n",
    "    if threshold is not None: indices = np.where(indices>=threshold, -1, indices)\n",
    "    return indices\n",
    "\n",
    "def find_oms_attr_for_lumisections(runs, lumis, omsjson, omsattr, **kwargs):\n",
    "    # check if attribute is present\n",
    "    if not omsattr in omsjson.keys():\n",
    "        msg = 'Attribute \"{}\" not found in provided omsjson.'.format(omsattr)\n",
    "        msg += ' Available keys are: {}'.format(omsjson.keys())\n",
    "        raise Exception(msg)\n",
    "    # retrieve indices for provided lumisections\n",
    "    indices = find_oms_indices(runs, lumis, omsjson, **kwargs)\n",
    "    # make array of values corresponding to indices\n",
    "    # (assume index -1 is used as a default for lumisections that are missing in the omsjson)\n",
    "    values = np.array([(omsjson[omsattr][idx] if idx>-1 else 0) for idx in indices])\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0444452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[386660 386661 386663 386665 386667 386668]\n"
     ]
    }
   ],
   "source": [
    "# Load parquet for Ring 2 Zero Bias 2024 era Iv1\n",
    "\n",
    "menames = ['Ring2']\n",
    "datadir = '../Development/data/'\n",
    "parquet = 'ZeroBias-Run2024I-PromptReco-v1-DQMIO-PixelPhase1-Phase1_MechanicalView-PXForward-clusters_per_SignedDiskCoord_per_SignedBladePanelCoord_PXRing_2.parquet'\n",
    "#parquet = 'ZeroBias-Run2025D-PromptReco-v1-DQMIO-PixelPhase1-Phase1_MechanicalView-PXForward-clusters_per_SignedDiskCoord_per_SignedBladePanelCoord_PXRing_2.parquet'\n",
    "\n",
    "X = {}\n",
    "for mename in menames:\n",
    "    f = os.path.join(datadir, parquet)\n",
    "    X[mename] = read_parquet(f, verbose=False, batch_size=750, batch_ids=[29])\n",
    "#    X[mename] = read_parquet(f, verbose=False, batch_size=750, batch_ids=[114])\n",
    "#    X[mename] = read_parquet(f, verbose=False, batch_size=2000, batch_ids=[42])\n",
    "    \n",
    "# Print run numbers\n",
    "run_numbers = X[menames[0]]['run_number'].values\n",
    "ls_numbers = X[menames[0]]['ls_number'].values\n",
    "runs = np.unique(run_numbers)\n",
    "print(runs)\n",
    "#print(np.unique(ls_numbers))\n",
    "#[386660 386661 386663 386665 386667 386668]\n",
    "\n",
    "#In This run range, there is only 1 Multi-Disk anomaly (Ring2 model find only LS 103) and 3 Single-Disk (Ring 2 model find all, but ignore)\n",
    "#386661 - 103 - 104 - 2 - Disk +1+2+3 - Multi-Disk\n",
    "#386661 - 303 - 305 - 3 - Disk +3 - Single-Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1dfdb4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract np arrays from dataframes\n",
    "\n",
    "X_data = {}\n",
    "for mename in menames:\n",
    "    mes, _, _ = get_mes(X[mename],\n",
    "                    xbinscolumn='x_bin', ybinscolumn='y_bin',\n",
    "                    runcolumn='run_number', lumicolumn='ls_number')\n",
    "    X_data[mename] = mes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24d1c328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filter info:\n",
      "  - oms__pileup: (750,)\n",
      "  - oms__beams_stable: (750,)\n",
      "  - oms__cms_active: (750,)\n",
      "  - oms__bpix_ready: (750,)\n",
      "  - oms__fpix_ready: (750,)\n",
      "  - oms__tibtid_ready: (750,)\n",
      "  - oms__tob_ready: (750,)\n",
      "  - oms__tecp_ready: (750,)\n",
      "  - oms__tecm_ready: (750,)\n",
      "  - oms__run_number: (750,)\n",
      "  - oms__lumisection_number: (750,)\n",
      "Input data keys:\n",
      "dict_keys(['Ring2', 'oms__pileup', 'oms__beams_stable', 'oms__cms_active', 'oms__bpix_ready', 'oms__fpix_ready', 'oms__tibtid_ready', 'oms__tob_ready', 'oms__tecp_ready', 'oms__tecm_ready', 'oms__run_number', 'oms__lumisection_number'])\n"
     ]
    }
   ],
   "source": [
    "# Load OMS data\n",
    "oms_data_dir = '../Development/omsdata/'\n",
    "oms_info_file = f'{oms_data_dir}omsdata_Run2024I-v1.json'\n",
    "#oms_info_file = f'{oms_data_dir}omsdata_Run2025D-v1.json'\n",
    "with open(oms_info_file, 'r') as f:\n",
    "    oms_info = json.load(f)\n",
    "oms_attrs = [\n",
    "    \"beams_stable\",\n",
    "    \"cms_active\",\n",
    "    \"bpix_ready\",\n",
    "    \"fpix_ready\",\n",
    "    \"tibtid_ready\",\n",
    "    \"tob_ready\",\n",
    "    \"tecp_ready\",\n",
    "    \"tecm_ready\",\n",
    "    \"pileup\"\n",
    "]\n",
    "oms_info_new = {}\n",
    "for key, val in oms_info.items():\n",
    "    if key not in oms_attrs: continue\n",
    "    oms_info_new['oms__' + key] = find_oms_attr_for_lumisections(run_numbers, ls_numbers, oms_info, key)\n",
    "oms_info = oms_info_new\n",
    "\n",
    "# add filter info to input data\n",
    "filter_info = {**oms_info}\n",
    "filter_info['oms__run_number'] = run_numbers\n",
    "filter_info['oms__lumisection_number'] = ls_numbers\n",
    "print('Filter info:')\n",
    "for key, val in filter_info.items():\n",
    "    print(f'  - {key}: {val.shape}')\n",
    "X_data.update(filter_info)\n",
    "print('Input data keys:')\n",
    "print(X_data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5432a546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will write following arrays:\n",
      "  - Ring2: (750, 140, 56)\n",
      "  - run_number: (750,)\n",
      "  - ls_number: (750,)\n"
     ]
    }
   ],
   "source": [
    "# Store data in pickle format\n",
    "import pickle    \n",
    "    \n",
    "# add the run and lumisection numbers to the data to be stored in .pkl file\n",
    "X_data_towrite = {}\n",
    "for key, val in X_data.items():\n",
    "    if key.startswith('oms__'): continue # do not save OMS data to .pkl (will be retrieved on the fly by DIALS)\n",
    "    X_data_towrite[key] = val\n",
    "X_data_towrite['run_number'] = run_numbers\n",
    "X_data_towrite['ls_number'] = ls_numbers\n",
    "\n",
    "# printouts for checking\n",
    "print('Will write following arrays:')\n",
    "for key, val in X_data_towrite.items():\n",
    "    print(f'  - {key}: {val.shape}')\n",
    "\n",
    "with open('test_data.pkl', 'wb') as f:\n",
    "    pickle.dump(X_data_towrite, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d042d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys: ['Ring2', 'run_number', 'ls_number']\n",
      "Ring2: shape = (750, 140, 56), dtype = float64\n",
      "run_number: shape = (750,), dtype = int64\n",
      "ls_number: shape = (750,), dtype = int64\n"
     ]
    }
   ],
   "source": [
    "# Load the picke file (check file content)\n",
    "with open(\"test_data.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "# Print dictionary keys\n",
    "print(\"Keys:\", list(data.keys()))\n",
    "\n",
    "# Print shape e type of each element\n",
    "for key, value in data.items():\n",
    "    print(f\"{key}: shape = {value.shape}, dtype = {value.dtype}\")"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
