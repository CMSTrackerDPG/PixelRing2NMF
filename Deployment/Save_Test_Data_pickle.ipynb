{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7215116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import external modules\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7fa0b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import parquet and 2 functions\n",
    "\n",
    "import pyarrow as pa\n",
    "from pyarrow.parquet import ParquetFile, ParquetDataset\n",
    "\n",
    "def read_parquet(path, verbose=False, \n",
    "                 columns=None, batch_size=None, first_batch=None, last_batch=None, batch_ids=None):\n",
    "    if batch_size is None:\n",
    "        # standard case where all rows are read\n",
    "        df = pd.read_parquet(path, columns=columns)\n",
    "    else:\n",
    "        # more involved case where only a section of rows is read\n",
    "        pf = ParquetFile(path)\n",
    "        if verbose:\n",
    "            print('Found following parquet metadata:')\n",
    "            print(pf.metadata)\n",
    "            \n",
    "        # check if contradictory arguments were provided\n",
    "        if batch_ids is not None:\n",
    "            if first_batch is not None or last_batch is not None:\n",
    "                if verbose:\n",
    "                    msg = 'WARNING in read_parquet: cannot provide both batch_ids and first_batch / last_batch;'\n",
    "                    msg += ' first_batch and last_batch will be ignored.'\n",
    "                    print(msg)\n",
    "        else:\n",
    "            if first_batch is None or last_batch is None:\n",
    "                msg = 'ERROR in read_parquet: in batched mode, either batch_ids, or first_batch and last_batch'\n",
    "                msg += ' must be provided; returning None.'\n",
    "                print(msg)\n",
    "                return None\n",
    "            if last_batch < first_batch:\n",
    "                last_batch = first_batch\n",
    "                if verbose:\n",
    "                    msg = f'WARNING in read_parquet: setting last_batch to {last_batch}'\n",
    "                    msg += ' as values smaller than first_batch are not supported.'\n",
    "                    print(msg)\n",
    "            batch_ids = list(range(first_batch, last_batch+1))\n",
    "        \n",
    "        # check available rows and batches\n",
    "        num_rows = pf.metadata.num_rows\n",
    "        num_batches = int((num_rows-1)/batch_size)+1\n",
    "        if max(batch_ids) >= num_batches:\n",
    "            if verbose:\n",
    "                msg = f'WARNING in read_parquet: batch indices greater than {num_batches-1} will be ignored.'\n",
    "                print(msg)\n",
    "        \n",
    "        # iterate through the batches\n",
    "        iterobj = pf.iter_batches(batch_size = batch_size)\n",
    "        batches = []\n",
    "        for batch_idx in range(num_batches):\n",
    "            batch = next(iterobj)\n",
    "            if batch_idx in batch_ids: batches.append(batch)\n",
    "        df = pa.Table.from_batches(batches).to_pandas()\n",
    "        \n",
    "    if verbose:\n",
    "        print(f'Read dataframe with {len(df)} rows and {len(df.columns)} columns.')\n",
    "    return df\n",
    "\n",
    "def get_mes(df, datacolumn='data', xbinscolumn='xbins', ybinscolumn='ybins',\n",
    "            runcolumn='run', lumicolumn='lumi',\n",
    "            runs=None, lumis=None):\n",
    "    if runs is not None: df = select_runs(df, runs, runcolumn=runcolumn)\n",
    "    if lumis is not None: df = select_ls(df, lumis, lumicolumn=lumicolumn)\n",
    "    xbins = int(df[xbinscolumn].values[0])\n",
    "    ybins = int(df[ybinscolumn].values[0])\n",
    "    # note: df['data'][idx] yields an array of 1d arrays;\n",
    "    # need to convert it to a 2d array with np.stack\n",
    "    mes = np.array([np.stack(df[datacolumn].values[i]).reshape(ybins,xbins) for i in range(len(df))])\n",
    "    runs = df[runcolumn].values\n",
    "    lumis = df[lumicolumn].values\n",
    "    return (mes, runs, lumis)\n",
    "\n",
    "def find_oms_indices(runs, lumis, omsjson,\n",
    "                     run_key='run_number', lumi_key='lumisection_number',\n",
    "                     verbose=True):\n",
    "    # check if run_key and lumi_key are present in the omsjson\n",
    "    if not run_key in omsjson.keys():\n",
    "        msg = 'Run key \"{}\" not found in provided omsjson.'.format(run_key)\n",
    "        msg += ' Available keys are: {}'.format(omsjson.keys())\n",
    "        raise Exception(msg)\n",
    "    if not lumi_key in omsjson.keys():\n",
    "        msg = 'Lumi key \"{}\" not found in provided omsjson.'.format(lumi_key)\n",
    "        msg += ' Available keys are: {}'.format(omsjson.keys())\n",
    "        raise Exception(msg)\n",
    "    # parse runs and lumis\n",
    "    runs = np.array(runs).astype(int)\n",
    "    lumis = np.array(lumis).astype(int)\n",
    "    if len(runs.shape)!=1:\n",
    "        raise Exception(f'Provided run numbers array has unexpected shape: {runs.shape}')\n",
    "    if len(lumis.shape)!=1:\n",
    "        raise Exception(f'Provided lumisection numbers array has unexpected shape: {lumis.shape}')\n",
    "    # make sure provided oms runs and lumis are in the same data format\n",
    "    omsruns = np.array(omsjson[run_key]).astype(int)\n",
    "    omslumis = np.array(omsjson[lumi_key]).astype(int)\n",
    "    # combine run and lumisection number into a single unique number\n",
    "    idfactor = 10000 # warning: do not use 1e4 instead of 10000 to avoid conversion from int to float\n",
    "    ids = runs*idfactor + lumis\n",
    "    ids = ids.astype(int)\n",
    "    omsids = omsruns*idfactor + omslumis\n",
    "    omsids = omsids.astype(int)\n",
    "    # check if all ids are in omsids\n",
    "    # note: reduce from error to warning,\n",
    "    # since it seems some lumisections are intrinsically missing in OMS,\n",
    "    # e.g. run 380147, LS 186.\n",
    "    threshold = None\n",
    "    if np.any(np.isin(ids, omsids, invert=True)):\n",
    "        missing_ids_inds = np.isin(ids, omsids, invert=True).nonzero()[0]\n",
    "        missing_ids = ids[missing_ids_inds]\n",
    "        if verbose:\n",
    "            msg = 'WARNING: not all provided lumisections could be found in the oms data.'\n",
    "            msg += f' Missing lumisections are: {missing_ids}'\n",
    "            msg += f' ({len(missing_ids)} / {len(ids)})'\n",
    "            print(msg)\n",
    "        # temporarily add the missing ids to omsids\n",
    "        # (corresponding indices will be set to -1 later)\n",
    "        threshold = len(omsids)\n",
    "        omsids = np.concatenate((omsids, missing_ids))\n",
    "    # find indices of ids in omsids\n",
    "    omsids_sorted_inds = np.argsort(omsids)\n",
    "    omsids_sorted = omsids[omsids_sorted_inds]\n",
    "    indices = np.searchsorted(omsids_sorted, ids, side='left')\n",
    "    indices = omsids_sorted_inds[indices]\n",
    "    if threshold is not None: indices = np.where(indices>=threshold, -1, indices)\n",
    "    return indices\n",
    "\n",
    "def find_oms_attr_for_lumisections(runs, lumis, omsjson, omsattr, **kwargs):\n",
    "    # check if attribute is present\n",
    "    if not omsattr in omsjson.keys():\n",
    "        msg = 'Attribute \"{}\" not found in provided omsjson.'.format(omsattr)\n",
    "        msg += ' Available keys are: {}'.format(omsjson.keys())\n",
    "        raise Exception(msg)\n",
    "    # retrieve indices for provided lumisections\n",
    "    indices = find_oms_indices(runs, lumis, omsjson, **kwargs)\n",
    "    # make array of values corresponding to indices\n",
    "    # (assume index -1 is used as a default for lumisections that are missing in the omsjson)\n",
    "    values = np.array([(omsjson[omsattr][idx] if idx>-1 else 0) for idx in indices])\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a0444452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[395419 395422 395424 395425 395426 395428 395429 395430 395431 395434\n",
      " 395435 395437 395439 395440 395442 395443 395444 395445]\n",
      "[  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n",
      "  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36\n",
      "  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54\n",
      "  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72\n",
      "  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90\n",
      "  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108\n",
      " 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126\n",
      " 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144\n",
      " 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162\n",
      " 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180\n",
      " 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198\n",
      " 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216\n",
      " 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234\n",
      " 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252\n",
      " 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270\n",
      " 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288\n",
      " 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306\n",
      " 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324\n",
      " 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342\n",
      " 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360\n",
      " 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378\n",
      " 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396\n",
      " 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414\n",
      " 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432\n",
      " 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450\n",
      " 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468\n",
      " 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486\n",
      " 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504\n",
      " 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522\n",
      " 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540\n",
      " 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558\n",
      " 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576\n",
      " 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594\n",
      " 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612\n",
      " 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630\n",
      " 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648\n",
      " 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666\n",
      " 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684\n",
      " 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702\n",
      " 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720\n",
      " 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738\n",
      " 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756\n",
      " 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774\n",
      " 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792\n",
      " 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810\n",
      " 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828\n",
      " 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846\n",
      " 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864\n",
      " 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882\n",
      " 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900\n",
      " 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918\n",
      " 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936\n",
      " 937 938 939 940 941 942 943 944 945 946 947]\n"
     ]
    }
   ],
   "source": [
    "# Load parquet for Ring 2 Zero Bias 2024 era Iv1\n",
    "\n",
    "menames = ['Ring2']\n",
    "datadir = '/eos/user/a/alaperto/SWAN_projects/NMFolder/data/'\n",
    "parquet = 'ZeroBias-Run2024I-PromptReco-v1-DQMIO-PixelPhase1-Phase1_MechanicalView-PXForward-clusters_per_SignedDiskCoord_per_SignedBladePanelCoord_PXRing_2.parquet'\n",
    "#parquet = 'ZeroBias-Run2025D-PromptReco-v1-DQMIO-PixelPhase1-Phase1_MechanicalView-PXForward-clusters_per_SignedDiskCoord_per_SignedBladePanelCoord_PXRing_2.parquet'\n",
    "\n",
    "X = {}\n",
    "for mename in menames:\n",
    "    f = os.path.join(datadir, parquet)\n",
    "    X[mename] = read_parquet(f, verbose=False, batch_size=750, batch_ids=[29])\n",
    "#    X[mename] = read_parquet(f, verbose=False, batch_size=750, batch_ids=[114])\n",
    "#    X[mename] = read_parquet(f, verbose=False, batch_size=2000, batch_ids=[42])\n",
    "    \n",
    "# Print run numbers\n",
    "run_numbers = X[menames[0]]['run_number'].values\n",
    "ls_numbers = X[menames[0]]['ls_number'].values\n",
    "runs = np.unique(run_numbers)\n",
    "print(runs)\n",
    "#print(np.unique(ls_numbers))\n",
    "#[386660 386661 386663 386665 386667 386668]\n",
    "\n",
    "#In This run range, there is only 1 Multi-Disk anomaly (Ring2 model find only LS 103) and 3 Single-Disk (Ring 2 model find all, but ignore)\n",
    "#386661 - 103 - 104 - 2 - Disk +1+2+3 - Multi-Disk\n",
    "#386661 - 303 - 305 - 3 - Disk +3 - Single-Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1dfdb4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract np arrays from dataframes\n",
    "\n",
    "X_data = {}\n",
    "for mename in menames:\n",
    "    mes, _, _ = get_mes(X[mename],\n",
    "                    xbinscolumn='x_bin', ybinscolumn='y_bin',\n",
    "                    runcolumn='run_number', lumicolumn='ls_number')\n",
    "    X_data[mename] = mes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "24d1c328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filter info:\n",
      "  - oms__pileup: (2000,)\n",
      "  - oms__beams_stable: (2000,)\n",
      "  - oms__cms_active: (2000,)\n",
      "  - oms__bpix_ready: (2000,)\n",
      "  - oms__fpix_ready: (2000,)\n",
      "  - oms__tibtid_ready: (2000,)\n",
      "  - oms__tob_ready: (2000,)\n",
      "  - oms__tecp_ready: (2000,)\n",
      "  - oms__tecm_ready: (2000,)\n",
      "  - oms__run_number: (2000,)\n",
      "  - oms__lumisection_number: (2000,)\n",
      "Input data keys:\n",
      "dict_keys(['Ring2', 'oms__pileup', 'oms__beams_stable', 'oms__cms_active', 'oms__bpix_ready', 'oms__fpix_ready', 'oms__tibtid_ready', 'oms__tob_ready', 'oms__tecp_ready', 'oms__tecm_ready', 'oms__run_number', 'oms__lumisection_number'])\n"
     ]
    }
   ],
   "source": [
    "# Load OMS data\n",
    "oms_data_dir = '/eos/user/a/alaperto/SWAN_projects/NMFolder/omsdata/'\n",
    "#oms_info_file = f'{oms_data_dir}omsdata_Run2024I-v1.json'\n",
    "oms_info_file = f'{oms_data_dir}omsdata_Run2025D-v1.json'\n",
    "with open(oms_info_file, 'r') as f:\n",
    "    oms_info = json.load(f)\n",
    "oms_attrs = [\n",
    "    \"beams_stable\",\n",
    "    \"cms_active\",\n",
    "    \"bpix_ready\",\n",
    "    \"fpix_ready\",\n",
    "    \"tibtid_ready\",\n",
    "    \"tob_ready\",\n",
    "    \"tecp_ready\",\n",
    "    \"tecm_ready\",\n",
    "    \"pileup\"\n",
    "]\n",
    "oms_info_new = {}\n",
    "for key, val in oms_info.items():\n",
    "    if key not in oms_attrs: continue\n",
    "    oms_info_new['oms__' + key] = find_oms_attr_for_lumisections(run_numbers, ls_numbers, oms_info, key)\n",
    "oms_info = oms_info_new\n",
    "\n",
    "# add filter info to input data\n",
    "filter_info = {**oms_info}\n",
    "filter_info['oms__run_number'] = run_numbers\n",
    "filter_info['oms__lumisection_number'] = ls_numbers\n",
    "print('Filter info:')\n",
    "for key, val in filter_info.items():\n",
    "    print(f'  - {key}: {val.shape}')\n",
    "X_data.update(filter_info)\n",
    "print('Input data keys:')\n",
    "print(X_data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5432a546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will write following arrays:\n",
      "  - Ring2: (2000, 140, 56)\n",
      "  - run_number: (2000,)\n",
      "  - ls_number: (2000,)\n"
     ]
    }
   ],
   "source": [
    "# Store data in pickle format\n",
    "import pickle    \n",
    "    \n",
    "# add the run and lumisection numbers to the data to be stored in .pkl file\n",
    "X_data_towrite = {}\n",
    "for key, val in X_data.items():\n",
    "    if key.startswith('oms__'): continue # do not save OMS data to .pkl (will be retrieved on the fly by DIALS)\n",
    "    X_data_towrite[key] = val\n",
    "X_data_towrite['run_number'] = run_numbers\n",
    "X_data_towrite['ls_number'] = ls_numbers\n",
    "\n",
    "# printouts for checking\n",
    "print('Will write following arrays:')\n",
    "for key, val in X_data_towrite.items():\n",
    "    print(f'  - {key}: {val.shape}')\n",
    "\n",
    "with open('test_data.pkl', 'wb') as f:\n",
    "    pickle.dump(X_data_towrite, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3d042d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys: ['Ring2', 'run_number', 'ls_number']\n",
      "Ring2: shape = (2000, 140, 56), dtype = float64\n",
      "run_number: shape = (2000,), dtype = int64\n",
      "ls_number: shape = (2000,), dtype = int64\n"
     ]
    }
   ],
   "source": [
    "# Load the picke file (check file content)\n",
    "with open(\"test_data.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "# Print dictionary keys\n",
    "print(\"Keys:\", list(data.keys()))\n",
    "\n",
    "# Print shape e type of each element\n",
    "for key, value in data.items():\n",
    "    print(f\"{key}: shape = {value.shape}, dtype = {value.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ff587a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
